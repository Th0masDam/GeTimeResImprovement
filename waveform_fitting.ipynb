{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf4027f7-e8c6-4047-aacf-b7d0b1d746d1",
   "metadata": {},
   "source": [
    "# Model testing\n",
    "\n",
    "https://www.youtube.com/watch?v=BoaHul6TXCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d8d53b-fcc5-4221-9e41-916e6f465935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly import graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import cufflinks\n",
    "import numba as nb\n",
    "from scipy.optimize import least_squares, curve_fit\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.svm import SVC # Support Vector Classifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import mlflow\n",
    "import os\n",
    "import mlflow.keras\n",
    "import mlflow.sklearn\n",
    "from gewapro.cache import cache\n",
    "from gewapro.preprocessing import get_waveforms, train_test_split_cond, smoothen_waveforms, get_and_smoothen_waveforms, select_from_source\n",
    "from gewapro.functions import (quadratic_arr,\n",
    "                               fit_parabolas,\n",
    "                               df_with_fits,\n",
    "                               _fit_final_slope,\n",
    "                               combine_and, combine_or,\n",
    "                               calc_ab)\n",
    "from gewapro.plotting.base import _fwhm_energy_df\n",
    "from gewapro.plotting import (histogram,\n",
    "                              corr_fig,\n",
    "                              mlp_reg_fig,\n",
    "                              plot_transform,\n",
    "                              energy_histogram,\n",
    "                              boxplot,\n",
    "                              plot_predictions,\n",
    "                              energy_line_plot,\n",
    "                              rmse_energy_line_plot)\n",
    "from gewapro.models import regressor_model, train_model, get_model_version_map, ModelInfo\n",
    "from gewapro.experiment_flow import run_experiment\n",
    "import mlflow.pyfunc\n",
    "import xgboost as xgb\n",
    "import itertools\n",
    "\n",
    "cufflinks.go_offline()\n",
    "\n",
    "# data06_name = \"20231110-Na22-d0-12-ML-Tz6-100ns-ecf-06.csv\"\n",
    "# data07_name = \"20231110-Na22-d0-12-ML-Tz6-100ns-ecf-07.csv\"\n",
    "# data08_name = \"20231110-Na22-d0-12-ML-Tz6-100ns-ecf-08.csv\"\n",
    "# data10_name = \"20231110-Na22-d0-12-ML-Tz6-100ns-ecf-10.csv\"\n",
    "# data20_name = \"20231110-Na22-d0-12-ML-Tz6-100ns-ecf-20.csv\"\n",
    "# data30_name = \"20231110-Na22-d0-12-ML-Tz6-100ns-ecf-30.csv\"\n",
    "# data40_name = \"20231110-Na22-d0-12-ML-Tz6-100ns-ecf-40.csv\"\n",
    "# data50_name = \"20231110-Na22-d0-12-ML-Tz6-100ns-ecf-50.csv\"\n",
    "# data60_name = \"20231110-Na22-d0-12-ML-Tz6-100ns-ecf-60.csv\"\n",
    "data_g1274_name = \"20231110-Na22-d0-12-tz6-ML200-g1274.dat\"\n",
    "data_g511_name = \"20231110-Na22-d0-12-tz6-ML200-g511.dat\"\n",
    "data_g1274_unfiltered_name = \"20231110-Na22-d0-12-tz6-ML200_nofir_noMa_D40-g1274.dat\"\n",
    "data_g511_unfiltered_name = \"20231110-Na22-d0-12-tz6-ML200_nofir_noMa_D40-g511.dat\"\n",
    "data_g1274_partfltr_name = \"20231110-Na22-d0-12-tz6-ML200_noMa_D10_soft_sc_g1274.dat\"\n",
    "data_g511_partfltr_name = \"20231110-Na22-d0-12-tz6-ML200_noMa_D10_soft_sc_g511.dat\"\n",
    "data_g1274_partfltr_name_all = \"20231110-Na22-d0-12-tz6-ML200_noMa_D10_soft_sc_g1274_all.dat\"\n",
    "data_g511_partfltr_name_all = \"20231110-Na22-d0-12-tz6-ML200_noMa_D10_soft_sc_g511_all.dat\"\n",
    "name_g1274_d = lambda i: f\"20231110-Na22-d{i}-tz6-ML200_noMa_D10_soft_sc_g1274.dat\"\n",
    "name_g511_d = lambda i: f\"20231110-Na22-d{i}-tz6-ML200_noMa_D10_soft_sc_g511.dat\"\n",
    "# data06 = pd.read_csv(\"data/\"+data06_name)\n",
    "# data07 = pd.read_csv(\"data/\"+data07_name)\n",
    "# data08 = pd.read_csv(\"data/\"+data08_name)\n",
    "# data10 = pd.read_csv(\"data/\"+data10_name)\n",
    "# data20 = pd.read_csv(\"data/\"+data20_name)\n",
    "# data30 = pd.read_csv(\"data/\"+data30_name)\n",
    "# data40 = pd.read_csv(\"data/\"+data40_name)\n",
    "# data50 = pd.read_csv(\"data/\"+data50_name)\n",
    "# data60 = pd.read_csv(\"data/\"+data60_name)\n",
    "datag1274 = pd.read_csv(\"data/\"+data_g1274_name)\n",
    "datag511 = pd.read_csv(\"data/\"+data_g511_name)\n",
    "datag1274unfiltered = pd.read_csv(\"data/\"+data_g1274_unfiltered_name)\n",
    "datag511unfiltered = pd.read_csv(\"data/\"+data_g511_unfiltered_name)\n",
    "datag1274partfltr = pd.read_csv(\"data/\"+data_g1274_partfltr_name)\n",
    "datag511partfltr = pd.read_csv(\"data/\"+data_g511_partfltr_name)\n",
    "datag1274partfltr_all = pd.read_csv(\"data/\"+data_g1274_partfltr_name_all)\n",
    "datag511partfltr_all = pd.read_csv(\"data/\"+data_g511_partfltr_name_all)\n",
    "data_dict = {\n",
    "            #  data06_name: data06,\n",
    "            #  data07_name:data07,\n",
    "            #  data08_name:data08,\n",
    "            #  data10_name:data10,\n",
    "            #  data20_name:data20,\n",
    "            #  data30_name:data30,\n",
    "            #  data40_name:data40,\n",
    "            #  data50_name:data50,\n",
    "            #  data60_name:data60,\n",
    "             data_g1274_name:data_g1274_name,\n",
    "             data_g511_name:data_g511_name,\n",
    "             data_g1274_unfiltered_name:datag1274unfiltered,\n",
    "             data_g511_unfiltered_name:datag511unfiltered,\n",
    "             data_g1274_partfltr_name:datag1274partfltr,\n",
    "             data_g511_partfltr_name:datag511partfltr,\n",
    "             data_g1274_partfltr_name_all:datag1274partfltr_all,\n",
    "             data_g511_partfltr_name_all:datag511partfltr_all,\n",
    "            } | {\n",
    "                    # name_g1274_d(i): select_from_source(datag1274partfltr_all, select_channels=[i]) for i in range(0,13)\n",
    "            } | {\n",
    "                    name_g511_d(i): select_from_source(datag511partfltr_all, select_channels=[i]) for i in range(0,13)\n",
    "            }\n",
    "\n",
    "display_all = 1\n",
    "if display_all:\n",
    "    # display(data_dict[name_g1274_d(1)])\n",
    "    # display(data_dict[name_g1274_d(2)])\n",
    "    # display(data_dict[name_g1274_d(0)])\n",
    "    # display(data_dict[data_g1274_unfiltered_name])\n",
    "    # display(data_dict[name_g511_d(0)])\n",
    "    # display(data_dict[data_g511_unfiltered_name])\n",
    "    pass\n",
    "    # display(data06) # Ch, E, dT, s0 ... s127, # 18662 rows/waveforms\n",
    "\n",
    "x_to_t = lambda x: 160-(x*4)\n",
    "t_to_x = lambda t: (160-t)/4\n",
    "# def t_to_x(t): return (160 - t) / 4\n",
    "\n",
    "unnormalized = []\n",
    "normalized = []\n",
    "for normalize in [True, False]:\n",
    "    for data,range_E in [(datag1274, (4000,5000)), (datag511, (11050,11250)), (datag1274unfiltered, (4000,5000)), (datag511unfiltered, (11050,11250)),\n",
    "                        (datag1274partfltr,(4000,5000)),(datag511partfltr,(11050,11250))]+[(dat,()) for dat in [datag1274,datag511,datag1274unfiltered,datag511unfiltered,datag1274partfltr,datag511partfltr]]:\n",
    "        data_in_E_range = get_waveforms(source_data=data, select_energies=range_E, include_energy=False)\n",
    "        if normalize:\n",
    "            normalized.append(data_in_E_range)\n",
    "        else:\n",
    "            s_labels_E = pd.Series(np.array([float([s[s.find(\"E\")+1:] for s in [col.replace(\" \",\"\")]][0]) for col in data_in_E_range.columns])) * .1139\n",
    "            data_in_E_range.loc[:] = s_labels_E.T.values * data_in_E_range\n",
    "            unnormalized.append(data_in_E_range)\n",
    "\n",
    "unnorm_fltr_dict = {\"g511\": unnormalized[0], \"g1274\": unnormalized[1], \"gBOTH\":{\"range_cut\":pd.concat(unnormalized[:2],axis=1), \"range_all\": pd.concat(unnormalized[6:8],axis=1)}}\n",
    "unnorm_unfltr_dict = {\"g511\": unnormalized[2], \"g1274\": unnormalized[3], \"gBOTH\":{\"range_cut\":pd.concat(unnormalized[2:4],axis=1), \"range_all\": pd.concat(unnormalized[8:10],axis=1)}}\n",
    "unnorm_partfltr_dict = {\"g511\": unnormalized[4], \"g1274\": unnormalized[5], \"gBOTH\":{\"range_cut\":pd.concat(unnormalized[4:6],axis=1), \"range_all\": pd.concat(unnormalized[10:],axis=1)}}\n",
    "norm_fltr_dict = {\"g511\": normalized[0], \"g1274\": normalized[1], \"gBOTH\":{\"range_cut\":pd.concat(normalized[:2],axis=1), \"range_all\": pd.concat(normalized[6:8],axis=1)}}\n",
    "norm_unfltr_dict = {\"g511\": normalized[2], \"g1274\": normalized[3], \"gBOTH\":{\"range_cut\":pd.concat(normalized[2:4],axis=1), \"range_all\": pd.concat(normalized[8:10],axis=1)}}\n",
    "norm_partfltr_dict = {\"g511\": normalized[4], \"g1274\": normalized[5], \"gBOTH\":{\"range_cut\":pd.concat(normalized[4:6],axis=1), \"range_all\": pd.concat(normalized[10:],axis=1)}}\n",
    "\n",
    "raw_dict = {\"unnormalized\": {\"filtered\": unnorm_fltr_dict, \"unfiltered\": unnorm_unfltr_dict, \"FIR\": unnorm_partfltr_dict, \"FIR_MA\": None},\n",
    "            \"normalized\": {\"filtered\": norm_fltr_dict, \"unfiltered\": norm_unfltr_dict, \"FIR\": norm_partfltr_dict, \"FIR_MA\": None} }\n",
    "data_dict[\"raw\"] = raw_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot = get_waveforms(select_energies=(),source_data=datag1274partfltr)\n",
    "print(\"Total waveform count:\",len(dfplot.columns))\n",
    "dfplot85_95 = dfplot.loc[:,(dfplot.loc[199] > 0.85) & (dfplot.loc[199] < 0.95)]\n",
    "dfplot85 = dfplot.loc[:,dfplot.loc[199] < 0.85]\n",
    "# dfplot.iloc[:,:100].iplot(title=\"First 100 waveforms\")\n",
    "# display(dfplot85_95)\n",
    "# dfplot85_95.iloc[:,:100].iplot(title=\"First 100 waveforms with last value between 0.85 and 0.95\")\n",
    "# display(dfplot85)\n",
    "# dfplot85.iloc[:,:100].iplot(title=\"First 100 waveforms with last value below 0.85\")\n",
    "print(f\"Final point <0.85 rate: {len(dfplot85.columns)/len(dfplot.columns):.2%} ({len(dfplot85.columns)} waveforms), 0.85-0.95 rate: {len(dfplot85_95.columns)/len(dfplot.columns):.2%} ({len(dfplot85_95.columns)} waveforms)\")\n",
    "data_dict[\"raw\"][\"unnormalized\"][\"unfiltered\"][\"gBOTH\"][\"range_all\"].iloc[:,-100:].iplot(title=\"Last 100 unfiltered unnormalized waveforms\")\n",
    "# data_dict[\"raw\"][\"normalized\"][\"FIR\"][\"gBOTH\"][\"range_cut\"].iloc[:,:50].iplot(title=\"First 50 partially filtered normalized waveforms E(4000-5000)(11050-11250)\")\n",
    "get_waveforms(source_data=data_dict[name_g511_d(0)], select_energies=(4000,5000)).iloc[:,:100].iplot(title=\"First 100 partially filtered normalized waveforms\")\n",
    "# data_dict[\"raw\"][\"normalized\"][\"partfltr\"][\"gBOTH\"][\"range_cut\"].iloc[:,-50:].iplot(title=\"Last 50 partially filtered normalized waveforms E(4000-5000)(11050-11250)\")\n",
    "# data_dict[\"raw\"][\"unnormalized\"][\"partfltr\"][\"gBOTH\"][\"range_cut\"].iloc[:,-50:].iplot(title=\"Last 50 partially filtered unnormalized waveforms E(4000-5000)(11050-11250)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modelinfo := ModelInfo.from_database(\"MLPRegressorModel\",3026))\n",
    "print(ModelInfo.from_database(\"MLPRegressorModeleerearwea2wefacd3qdwq\",3001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10dffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_diff = datag511partfltr_all[\"dT\"] - datag511partfltr_all[\"Tref\"]\n",
    "s_diff.name = \"dT - Tref\"\n",
    "display(pd.concat([datag511partfltr_all,s_diff],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d1897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "pca_components = 22\n",
    "hidden_layers = [16]\n",
    "activation = \"relu\"\n",
    "\n",
    "# Internal workings of the fitting\n",
    "data_df = get_waveforms(select_energies=(5000,50000),source_data=datag511partfltr) # all_data\n",
    "cutoff_val = 0.85\n",
    "data_df85: pd.DataFrame = data_df.loc[:,data_df.loc[199] > cutoff_val]\n",
    "print(f\"Discarding {len(data_df.columns)-len(data_df85.columns)} waveforms that end below {cutoff_val} from the data set (initially {len(data_df.columns)} waveforms)\")\n",
    "data = data_df85.values.transpose()\n",
    "labels_t = np.array([float([s[s.find(\"Tref\")+4:s.find(\",dT\")] for s in [col.replace(\" \",\"\")]][0]) for col in data_df85.columns])\n",
    "labels_x = t_to_x(labels_t)\n",
    "wave_i = np.array([int(col[col.find(\"[\")+1:col.find(\"]\")]) for col in data_df85.columns])\n",
    "\n",
    "# Transform the data (PCA/TruncatedSVD)\n",
    "pca_method = \"sklearn.decomposition.TruncatedSVD\"\n",
    "PCA_seed = round((592138171 * (datetime.now().timestamp()*9732103 % 38045729)) % 3244034593)\n",
    "model = TruncatedSVD(pca_components, random_state=PCA_seed)\n",
    "data_trans = model.fit_transform(data)\n",
    "pca_var_ratio = model.explained_variance_ratio_\n",
    "# print(\"pca_var_ratio:\",pca_var_ratio)\n",
    "\n",
    "# Create a conditioned train-test split on the data, with data not passing the condition added to the testing set\n",
    "d_train, d_test, l_train, l_test, l_train_t, l_test_t, wi_train, wi_test = train_test_split_cond(data_trans, labels_x, labels_t, wave_i, test_size=0.2, \n",
    "                                                                                                    random_state=42, \n",
    "                                                                                                    add_removed_to_test=True)\n",
    "print(\"[MLFlow run] Divided data in train, test sets:\",l_train.shape, l_test.shape,\" -> total set of\",l_train.shape[0]+l_test.shape[0],\"/ available\",\"unknown\")\n",
    "\n",
    "# Creating model\n",
    "regr = regressor_model(\"Tensorflow\", pca_components,hidden_layers,activation, max_iter=10, name=\"try_1\")#.summary()\n",
    "bst = regressor_model(\"XGBoost\", pca_components=22, max_depth=50, n_estimators=5, max_leaves=0) # Best hyperparameters for g511unfiltered: pca_components=22, max_depth=50, n_estimators=5\n",
    "try:\n",
    "    regr.summary()\n",
    "    try_ = True\n",
    "except:\n",
    "    try_ = False\n",
    "print(\"PCA seed:\",PCA_seed)\n",
    "regr if not try_ else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b36a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {data_g511_partfltr_name: datag511partfltr,\n",
    "        \"partfilter_g511_E8000-50000\":select_from_source(datag511partfltr,select_energies=(8000,50000)),\n",
    "        \"partfilter_g511_E5000-13000\":select_from_source(datag511partfltr,select_energies=(5000,13000)),\n",
    "        \"partfilter_g511_all_E5000-13000\":select_from_source(datag511partfltr_all,select_channels=[0,2,5,8,9,10],select_energies=(5000,13000))} | {\n",
    "        name_g511_d(ch): select_from_source(datag511partfltr_all,select_channels=[ch]) for ch in [0,1,2,3,4,5,7,8,9,10,11]}\n",
    "\n",
    "# energy_line_plot.clear_cache()\n",
    "print(energy_line_plot.cache_info())\n",
    "\n",
    "e_corrections = {0: calc_ab(4477,11197),\n",
    "                 1: calc_ab(4623,11538),\n",
    "                 2: calc_ab(4212,10512),\n",
    "                 3: calc_ab(4672,11662),\n",
    "                 4: (1,0),\n",
    "                 5: calc_ab(1582,3948),\n",
    "                 7: calc_ab(4747,11866),\n",
    "                 8: calc_ab(4303,10727),\n",
    "                 9: calc_ab(4750,11861),\n",
    "                 10:calc_ab(4113,10268),\n",
    "                 11: calc_ab(4474,11157)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy_line_plot.clear_cache()\n",
    "check_channels = [0,1,2]\n",
    "# plot_predictions(name_g511_d(0), (2450,2500), 2949, data, PCA_transform_on=\"partfilter_g511_E8000-50000\", verbose=1).show()\n",
    "# plot_predictions(name_g511_d(0), (6450,6500), 2949, data, PCA_transform_on=\"partfilter_g511_E8000-50000\", verbose=1).show()\n",
    "# plot_predictions(name_g511_d(0), (8450,8500), 2949, data, PCA_transform_on=\"partfilter_g511_E8000-50000\", verbose=1).show()\n",
    "# plot_predictions(name_g511_d(0), (6450,6500), 2851, data, PCA_transform_on=\"partfilter_g511_E5000-13000\", verbose=1).show()\n",
    "# plot_predictions(name_g511_d(0), (8450,8500), 2851, data, PCA_transform_on=\"partfilter_g511_E5000-13000\", verbose=1).show()\n",
    "# plot_predictions(name_g511_d(0), (9000,9250), 2949, data, PCA_transform_on=\"partfilter_g511_E8000-50000\", verbose=0).show()\n",
    "# plot_predictions(name_g511_d(2), (9000,9250), 2949, data, PCA_transform_on=\"partfilter_g511_E8000-50000\", verbose=0).show()\n",
    "# plot_predictions(name_g511_d(3), (9000,9250), 2949, data, PCA_transform_on=\"partfilter_g511_E8000-50000\", verbose=0).show()\n",
    "# plot_predictions(name_g511_d(7), (9000,9250), 2949, data, PCA_transform_on=\"partfilter_g511_E8000-50000\", verbose=0).show()\n",
    "# plot_predictions(name_g511_d(11), (9000,9250), 2949, data, PCA_transform_on=\"partfilter_g511_E8000-50000\", verbose=0).show()\n",
    "\n",
    "fig_eline = energy_line_plot(name_g511_d(0), 2000, 10000, 250, 2949, data, PCA_fit=\"partfilter_g511_E8000-50000\",hist_limit=100,y_sd=\"FWHM GoF\",verbose=0)\n",
    "fig_eline.show()\n",
    "# energy_line_plot(name_g511_d(0), 6000, 6500, 100, 2949, data, PCA_transform_on=\"partfilter_g511_E8000-50000\",hist_limit=100,y_sd=\"FWHM GoF\",verbose=0).show()\n",
    "# energy_line_plot.cache_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3e6e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145ad667",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gewapro.util import pandas_string_rep\n",
    "def combine_plots(fig_eline: go.Figure, fig_ehist: go.Figure, **layout_kwargs):\n",
    "    yaxis2_title = layout_kwargs.pop(\"yaxis2_title\", \"FWHM [ns]\")\n",
    "    for trace in fig_eline.data:\n",
    "        y2_max = 30\n",
    "        if trace.name == \"dT - Tref\":\n",
    "            y2_max = round(np.max(trace.y)+0.6)\n",
    "    fig_comb = make_subplots(specs=[[{\"secondary_y\": True}]]).update_layout(fig_ehist.layout).update_yaxes(title_text=yaxis2_title, range=[0,y2_max], secondary_y=True)\n",
    "    fig_eline.update_traces(yaxis=\"y2\")\n",
    "    fig_comb.add_traces(fig_eline.data+fig_ehist.data)\n",
    "    return fig_comb.update_layout(**layout_kwargs) if layout_kwargs else fig_comb\n",
    "\n",
    "def fitted_PCA(model_version: int, waveforms: pd.DataFrame, model_name: str = \"MLPRegressorModel\") -> PCA:\n",
    "    \"\"\"Gets a fitted_PCA for a certain model version, using the provided waveforms\"\"\"\n",
    "    start = datetime.now()\n",
    "    print(f\"Fitting data for {model_name} v{model_version} on {pandas_string_rep(waveforms)}\")\n",
    "    modelinfo: ModelInfo = ModelInfo.from_database(model_name=model_name,model_version=model_version)\n",
    "    pca: PCA = modelinfo.get_transformer()\n",
    "    pca.fit(waveforms.T.values)\n",
    "    print(f\"Fitting finished in\",datetime.now()-start)\n",
    "    return pca\n",
    "\n",
    "modelv_to_name = {2949:\"partfilter_g511_E8000-50000\"}\n",
    "modelv_to_name[2851] = modelv_to_name[2897] = \"partfilter_g511_E5000-13000\"\n",
    "modelv_to_name[2994] = fitted_PCA(2994, waveforms := get_waveforms(source_data=datag511partfltr_all, select_channels=[0,2,5,8,9,10], select_energies=(5000,13000)))\n",
    "modelv_to_name[2995] = fitted_PCA(2995, waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c95610",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v = 2995 #1: 2949, 2: 2851, 3: 2897\n",
    "\n",
    "for ch in [0,1,2,3,4,5,7]:\n",
    "    fig_eline = energy_line_plot(name_g511_d(ch), 100, 1300, 25, model_v, data, PCA_fit=modelv_to_name[model_v], correct_energy=e_corrections[ch],hist_limit=75,verbose=0)\n",
    "    fig_ehist = energy_histogram(name_g511_d(ch), data, select_energies=(0,1300), bins=[0,1400,2], correct_energy=e_corrections[ch], xaxis_title=\"Energy [keV]\")\n",
    "    combine_plots(fig_eline, fig_ehist).show()\n",
    "\n",
    "for ch in [8,9,10,11]:\n",
    "    fig_eline = energy_line_plot(name_g511_d(ch), 100, 1275, 25, model_v, data, PCA_fit=modelv_to_name[model_v], correct_energy=e_corrections[ch],hist_limit=75,verbose=0)\n",
    "    fig_ehist = energy_histogram(name_g511_d(ch), data, select_energies=(0,1300), bins=[0,1400,2], correct_energy=e_corrections[ch], xaxis_title=\"Energy [keV]\")\n",
    "    combine_plots(fig_eline, fig_ehist).show()\n",
    "#     plot_predictions(name_g511_d(ch), (10250,10350), 2949, data_dict=data, PCA_transform_on=\"partfilter_g511_E8000-50000\").show()\n",
    "#     plot_predictions(name_g511_d(ch), (10350,10450), 2949, data_dict=data, PCA_transform_on=\"partfilter_g511_E8000-50000\").show()\n",
    "#     plot_predictions(name_g511_d(ch), (10450,10550), 2949, data_dict=data, PCA_transform_on=\"partfilter_g511_E8000-50000\").show()\n",
    "#     plot_predictions(name_g511_d(ch), (10550,10650), 2949, data_dict=data, PCA_transform_on=\"partfilter_g511_E8000-50000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c1098",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 0\n",
    "fig0 = plot_predictions(name_g511_d(ch), (11638,12078), 2949, data_dict=data, PCA_transform_on=\"partfilter_g511_E8000-50000\",add_df=True)\n",
    "fig1 = plot_predictions(name_g511_d(ch), (11500,12000), 2949, data_dict=data, PCA_transform_on=\"partfilter_g511_E8000-50000\",add_df=True)\n",
    "fig2 = plot_predictions(name_g511_d(ch), (11350,12000), 2949, data_dict=data, PCA_transform_on=\"partfilter_g511_E8000-50000\",add_df=True)\n",
    "fig3 = plot_predictions(name_g511_d(ch), (11230,12000), 2949, data_dict=data, PCA_transform_on=\"partfilter_g511_E8000-50000\",add_df=True)\n",
    "fig4 = plot_predictions(name_g511_d(ch), (11224,12000), 2949, data_dict=data, PCA_transform_on=\"partfilter_g511_E8000-50000\",add_df=True)\n",
    "fig5 = plot_predictions(name_g511_d(ch), (11250,13000), 2949, data_dict=data, PCA_transform_on=\"partfilter_g511_E8000-50000\",add_df=True)\n",
    "for i,fig in enumerate([fig0,fig1,fig2,fig5,fig3,fig4]):\n",
    "    fig_params = {j:fig._params[col+\" Gaussian\"] for j,col in enumerate(fig._df.columns)}\n",
    "    for j,col in enumerate(fig._df.columns):\n",
    "        print(f\"Fig {i} goodness of fits   (col {j}, {len(fig._df)} observations):\\nx0: {fig_params[j]['x0']}, sigma: {fig_params[j]['sigma']}\")\n",
    "        min_tb,max_tb = fig_params[j][\"x0\"] - 5*fig_params[j][\"sigma\"], fig_params[j][\"x0\"] + 5*fig_params[j][\"sigma\"]\n",
    "        # print(\".5 :\",col,\":\",ksh := kstest(fig._df.loc[(fig._df[col] > min_tb) & (fig._df[col] < max_tb), col],cdf=norm.cdf,args=(fig_params[i][\"x0\"],0.5*fig_params[i][\"sigma\"])))\n",
    "        # print(\"1  :\",col,\":\",ks1 := kstest(fig._df.loc[(fig._df[col] > min_tb) & (fig._df[col] < max_tb), col],cdf=norm.cdf,args=(fig_params[i][\"x0\"],fig_params[i][\"sigma\"])))\n",
    "        # print(\"2  :\",col,\":\",ksd := kstest(fig._df.loc[(fig._df[col] > min_tb) & (fig._df[col] < max_tb), col],cdf=norm.cdf,args=(fig_params[i][\"x0\"],2*fig_params[i][\"sigma\"])))\n",
    "        # print(col,\"1/0.5:\",ks1.pvalue/ksh.pvalue,\"1/2:\",ks1.pvalue/ksd.pvalue)\n",
    "        # print(\"metric:\",col,fig_params[i][\"sigma\"]*((ks1.pvalue/ksh.pvalue)*(ks1.pvalue/ksd.pvalue))**(-0.25))\n",
    "        hist_params = histogram(fig._df,bins=[-30,30,0.25])._params[col+\" Gaussian\"]\n",
    "        # print(hist_params.keys(),np.sqrt(np.diag(hist_params[\"Covariance\"])),dict(zip(hist_params.keys(), np.sqrt(np.diag(hist_params[\"Covariance\"])))))\n",
    "        print(\"metric:\",col,hist_params[\"sigma\"]*hist_params[\"GoodnessOfFitMetric\"],\"\\n\",hist_params[\"GoodnessOfFitMetric\"])\n",
    "    fig.show()\n",
    "# histogram_ks(s_E_ranged,bins=[-30,30,0.25]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a39341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from functools import partial\n",
    "\n",
    "size = 100\n",
    "df_stat=pd.DataFrame(data={\"uniform\":(dist_unif := stats.uniform.rvs(size=size,loc=-1,scale=2)),\n",
    "                           \"normal\":(dist_norm := stats.norm.rvs(size=size)),\n",
    "                           \"normal_shifted\":(dist_nors := dist_norm+6),\n",
    "                           \"normal_shifted2\":(dist_nos2 := stats.norm.rvs(size=size, loc=6)),\n",
    "                           \"normal_shifted3\":(dist_nos3 := stats.norm.rvs(6,2,size=size)),\n",
    "                           \"normal_spread\":(dist_nos4 := stats.norm.rvs(0,5,size=size))})\n",
    "display(df_stat)\n",
    "print(stats.kstest(dist_unif,\n",
    "                   stats.norm.cdf),\"   for uniform dist\")\n",
    "print(stats.kstest(dist_norm,\n",
    "                   stats.norm.cdf),\"   for normal dist\")\n",
    "print(stats.kstest(rvs=dist_nors,\n",
    "                   cdf=stats.norm.cdf,\n",
    "                   args=(6,1)),\"   for normal shifted dist\")\n",
    "print(stats.kstest(rvs=dist_nos2,\n",
    "                   cdf=stats.norm.cdf,\n",
    "                   args=(6,1)),\"   for normal shifted2 dist\")\n",
    "print(stats.kstest(rvs=dist_nos3,\n",
    "                   cdf=stats.norm.cdf,\n",
    "                   args=(6,2)),\"   for normal shifted3 dist\")\n",
    "print(stats.kstest(dist_nos4,\n",
    "                   stats.norm.cdf,\n",
    "                   (0,5)).pvalue,\"   for normal spread dist\")\n",
    "histogram(df_stat,[-10,10,0.25]).show()\n",
    "df_stat.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c9909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gewapro.plotting.base import _get_ranges\n",
    "from gewapro.util import correct_energy, _validate_a_b, get_len, invert_start_end_step, correct_start_end_step\n",
    "\n",
    "slabelsE = pd.Series(s:=[123,2342,1231,4482,4232,2300,3203,4292,534,1230,8353],index=range(len(s)))\n",
    "# display(slabelsE)\n",
    "a,b = 0.17, 131\n",
    "start, end, step = [400,8177,101]\n",
    "(a,b),len_ = _validate_a_b((a,b)),get_len(start,end,step)\n",
    "print(\"original:\",start, end, step,\"periods:\", get_len(start,end,step))\n",
    "corrected_start_end_step = correct_start_end_step(start, end, step, a,b)\n",
    "print(\"corrected:\",corrected_start_end_step,\"periods:\", get_len(*corrected_start_end_step))\n",
    "increased_start_end_step = invert_start_end_step(*corrected_start_end_step, a,b)\n",
    "print(\"backtracked:\",increased_start_end_step,\"periods:\", get_len(*increased_start_end_step))\n",
    "slabelsEcorr = correct_energy((a,b),slabelsE)\n",
    "ranges = _get_ranges(start,end,step)\n",
    "ranges2 = _get_ranges(*corrected_start_end_step)\n",
    "print(ranges)\n",
    "print(ranges2)\n",
    "bins = {rang:((slabelsE >= rang[0]) & (slabelsE < rang[1])).sum() for rang in ranges}\n",
    "bins2 = {rang:((slabelsEcorr >= rang[0]) & (slabelsEcorr < rang[1])).sum() for rang in ranges2}\n",
    "print(bins)\n",
    "print(bins2)\n",
    "slabelsE = correct_energy((1/a,-b/a),correct_energy((a,b),slabelsE))\n",
    "dftestt = pd.DataFrame({\"old_bins\":bins.values(),\"new_bins\":bins2.values()})\n",
    "# dftestt.iplot()\n",
    "\n",
    "edf = _fwhm_energy_df(name_g511_d(0), 6000, 6500, 100, 2949, data, PCA_transform_on=\"partfilter_g511_E8000-50000\")\n",
    "display(edf)\n",
    "edf.index = pd.MultiIndex.from_tuples([(i[0], a*i[1]+b) for i in edf.index],names=edf.index.names)\n",
    "display(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75772b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting model\n",
    "# bst = train_model(bst, d_train, l_train)\n",
    "models_to_show = [] # [bst, regr]\n",
    "from gewapro.models import predict\n",
    "\n",
    "for model in models_to_show:\n",
    "# Combining labels and creating prediction Series\n",
    "    s_labels_t = pd.Series(np.append(l_train_t,l_test_t))\n",
    "    shift = -round(s_labels_t.mode().iloc[0])\n",
    "    s_labels_t.name = f\"Initial data: dT {'-' if shift < 0 else '+'} {abs(shift)} ns\"\n",
    "    predicted_train = predict(model, d_train)\n",
    "    print(predicted_train.shape, predicted_train)\n",
    "    pred_s_train = pd.Series(l_train_t - x_to_t(predicted_train),name=\"dT_act - dT_pred (train)\")\n",
    "    pred_s_test = pd.Series(l_test_t - x_to_t(predict(model, d_test)),name=\"dT_act - dT_pred (test)\")\n",
    "    pred_s = pd.Series(s_labels_t.values - x_to_t(predict(model, np.append(d_train,d_test, axis=0))),name=\"dT_act - dT_pred (both)\")\n",
    "    title = f\"Arrival time histogram for XGBoost regressor on g511\" # g511g1274 unfiltered combined data set (E range 4000-5000, 11050-11250)\"\n",
    "    # Add histogram with predicted vs actual data\n",
    "    fig_hist = histogram(pd.concat([s_labels_t+shift,pred_s_train,pred_s_test,pred_s], axis=1), [-30,30,0.25], xaxis_title=\"Time (ns)\", yaxis_title=\"Prevalence\", title=title, layout_width=1250)\n",
    "    print(f\"[MLFlow run] Created histogram with params: {fig_hist._params}\")\n",
    "    fig_hist.show()\n",
    "\n",
    "data_ = {\"data_g1274_partfltr\":datag1274partfltr,\"data_g511_partfltr\":datag511partfltr,\"exp_raw\":unnormalized_dict[all_data_partfltr_name],\"511_raw_train\":data_df}\n",
    "# display(datag511partfltr)\n",
    "layout_options = dict(yaxis_title=\"dT_act-dT_pred (ns)\", xaxis_title=\"Energy (arb. units)\") #plot_type=\"EnergyScatter\"\n",
    "plot_predictions(\"data_g511_partfltr\", (4550,14000), 369, data_, \"XGBoostedTree\", PCA_transform_on=\"511_raw_train\", **layout_options).show()\n",
    "plot_predictions(\"data_g511_partfltr\", (4550,14000), 370, data_, \"XGBoostedTree\", PCA_transform_on=\"511_raw_train\", **layout_options).show()\n",
    "# predict_from_model(\"data_g1274_partfltr\", (4000,5000), 366, data_, \"XGBoostedTree\", PCA_transform_on=\"exp_raw\").show()\n",
    "# predict_from_model(\"data_g511_partfltr\", (11050,11250), 366, data_, \"XGBoostedTree\", PCA_transform_on=\"exp_raw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "# PCA: 22 comp., 1261822223 seed\n",
    "# regr.save()\n",
    "# import keras\n",
    "# keras.saving.save_model(regr, \"data/models/reasonable_mixed_model.keras\")\n",
    "# regr = keras.saving.load_model(\"data/models/reasonable_mixed_model.keras\")\n",
    "import mlflow.xgboost\n",
    "\n",
    "save_model_run = True\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "experiment = mlflow.set_experiment(\"Sklearn NN, Na22 Ch0\")\n",
    "run_name = \"KerasNN[22]_nan%_xxx\"\n",
    "run_name = \"XGBoostedTree_nan%_xxx\"\n",
    "model = bst\n",
    "model_name = \"XGBoostedTree\"\n",
    "\n",
    "if save_model_run:\n",
    "    with mlflow.start_run(run_name=run_name) as mlflow_run:\n",
    "\n",
    "            # Log all parameters and metrics\n",
    "            mlflow.set_experiment_tag(\"BaseModel\",\"TensorFlow Keras Sequential Model\")\n",
    "            mlflow.log_params({\n",
    "                \"Channels used\": \"0\",\n",
    "                \"Energy range used\": \"ALL\",\n",
    "                \"Energy included for training\": False,\n",
    "                \"Applied conditions\": \"-\",\n",
    "                \"Used conditionally removed data in test set\": \"-\",\n",
    "                \"Train - Test set shapes\": f'{l_train_t.shape} - {l_test_t.shape}'.replace(\"(\",\"[\").replace(\",)\",\"]\").replace(\",\",\"\").replace(\")\",\"]\"),\n",
    "                \"Waveforms used\": f'{l_train_t.shape[0]+l_test_t.shape[0]} / available unknown',\n",
    "                \"Waveform smoothing\": \"None\",\n",
    "                \"Smoothing energy range\": \"-\",\n",
    "                \"PCA components\": pca_components,\n",
    "                \"PCA random seed\": PCA_seed,\n",
    "                \"PCA explained variance\": pca_var_ratio,\n",
    "                \"Hidden layers\": str(hidden_layers).replace(\",\",\" \"), \n",
    "                \"Activation function\": activation,\n",
    "                \"Solver\": \"rmsprop\",\n",
    "                \"Alpha\": \"-\",\n",
    "                \"Max epochs\": 10,\n",
    "            })\n",
    "            # Log the model\n",
    "            model_info = mlflow.xgboost.log_model(\n",
    "                xgb_model=model,\n",
    "                artifact_path=\"xgboost_models\",\n",
    "                signature=infer_signature(d_train, predicted_train),\n",
    "                input_example=np.array([d_train[0]]),\n",
    "                registered_model_name=model_name,\n",
    "                metadata={\"PCA random seed\": PCA_seed,\n",
    "                          \"PCA method\": pca_method},\n",
    "                )\n",
    "            # model_info = mlflow.sklearn.log_model(\n",
    "            #     sk_model=regr,\n",
    "            #     artifact_path=\"sk_models\",\n",
    "            #     signature=infer_signature(d_train, predicted_train),\n",
    "            #     input_example=d_train,\n",
    "            #     registered_model_name=\"MLPRegressorModel\",\n",
    "            #     metadata={\"PCA random seed\": PCA_seed}\n",
    "            # )\n",
    "# plot_predictions(exp_g511_unfiltered_name, (10350,14000), 213, combined_dict, \"XGBoostedTree\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb0665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCAcomp.  BEST / WORST (g511)     BEST / WORST (g1274)\n",
    "# 20:       2353 / 2355             2373 / 2370\n",
    "# 21:       2360 / 2357             2374 / 2379\n",
    "# 22:       2361 / 2365             2383 / 2384\n",
    "# df_results = pd.DataFrame({\"model\":[2353, 2360, 2361, 2373, 2374, 2383, 2355, 2357, 2365, 2370, 2379, 2384],\n",
    "#                            \"PCA components\":[20, 21, 22]*4,\"_g511\": [np.nan]*12,\"FWHM_g1274\": [np.nan]*12,\n",
    "#                            \"FWHM_g511 (450-600)\": [np.nan]*12,\"FWHM_g1274 (450-600)\": [np.nan]*12,\n",
    "#                            \"trained_on\":([\"g511\"]*3+[\"g1274\"]*3)*2,\n",
    "#                            }).set_index(\"model\")#rename_axis(\"limbs\", axis=\"columns\")\n",
    "# for model in df_results.index:\n",
    "#     model_pred_fig_511 = plot_predictions(data_g511_name, (11050,11250), model, data_dict, \"MLPRegressorModel\", False)\n",
    "#     model_pred_fig_1274 = plot_predictions(data_g1274_name, (4000,5000), model, data_dict, \"MLPRegressorModel\", False)\n",
    "#     model_pred_fig_511_515 = plot_predictions(data_g511_name, (450,600), model, data_dict, \"MLPRegressorModel\", False)\n",
    "#     model_pred_fig_1274_515 = plot_predictions(data_g1274_name, (450,600), model, data_dict, \"MLPRegressorModel\", False)\n",
    "#     df_results.loc[model, \"FWHM_g511\"] = model_pred_fig_511._params[\"dT_act - dT_pred Gaussian\"][\"sigma\"]*2*np.sqrt(2*np.log(2))\n",
    "#     df_results.loc[model, \"FWHM_g1274\"] = model_pred_fig_1274._params[\"dT_act - dT_pred Gaussian\"][\"sigma\"]*2*np.sqrt(2*np.log(2))\n",
    "#     df_results.loc[model, \"FWHM_g511 (450-600)\"] = model_pred_fig_511_515._params[\"dT_act - dT_pred Gaussian\"][\"sigma\"]*2*np.sqrt(2*np.log(2))\n",
    "#     df_results.loc[model, \"FWHM_g1274 (450-600)\"] = model_pred_fig_1274_515._params[\"dT_act - dT_pred Gaussian\"][\"sigma\"]*2*np.sqrt(2*np.log(2))\n",
    "# display(df_results)\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def lineariser(df: pd.DataFrame, lin_term: float, bias: float = 0):\n",
    "    return (df.max().values * lin_term) + bias\n",
    "\n",
    "def part_lin(lin_term: float, bias: float = 0):\n",
    "    return partial(lineariser, lin_term=lin_term, bias=bias) #custom_func=part_lin(0.01)\n",
    "\n",
    "#  511 * x - 17.8370 + y = 0  -   (0.034906 * 511 = )\n",
    "# 1274 * x +  8.6423 + y = 0  +     ()\n",
    "# =============================\n",
    "#  763 * x + 26.4793     = 0    -> x,y = -0.034704194,35.57084313\n",
    "import mlflow.pyfunc\n",
    "regressor = mlflow.pyfunc.load_model(model_uri=f\"models:/MLPRegressorModel/2398\")\n",
    "# print(isinstance(regressor, mlflow.pyfunc.PyFuncModel), str(regressor.loader_module) == \"mlflow.sklearn\")\n",
    "\n",
    "combined_model_v = 2411 # 2388, 2406, 2402, best: 2411 (worth a try?: 2447, 2464)\n",
    "predict_dict = {\"gBOTH_raw_normalized_partfltr_(4k,5k)(11.05k,11.25k)\": data_dict[\"raw\"][\"normalized\"][\"partfltr\"][\"gBOTH\"][\"range_cut\"],\n",
    "                \"gBOTH_raw_normalized_filtered_(4k,5k)(11.05k,11.25k)\": data_dict[\"raw\"][\"normalized\"][\"filtered\"][\"gBOTH\"][\"range_cut\"],\n",
    "                \"g511_raw_normalized_filtered\": data_dict[\"raw\"][\"normalized\"][\"filtered\"][\"g511\"],\n",
    "                \"g1274_raw_normalized_filtered\": data_dict[\"raw\"][\"normalized\"][\"filtered\"][\"g1274\"]}\n",
    "plot_predictions(\"gBOTH_raw_normalized_filtered_(4k,5k)(11.05k,11.25k)\", \"ALL\", combined_model_v, predict_dict, \"MLPRegressorModel\").show()\n",
    "plot_predictions(\"g1274_raw_normalized_filtered\", (4000,5000), combined_model_v, predict_dict, \"MLPRegressorModel\", PCA_transform_on=\"gBOTH_raw_normalized_filtered_(4k,5k)(11.05k,11.25k)\").show()\n",
    "plot_predictions(\"g511_raw_normalized_filtered\", (11050,11250), combined_model_v, predict_dict, \"MLPRegressorModel\", PCA_transform_on=\"gBOTH_raw_normalized_filtered_(4k,5k)(11.05k,11.25k)\").show()\n",
    "print(\"\\n\\n\\n\\n\\nnew tests:\")\n",
    "plot_predictions(\"gBOTH_raw_normalized_partfltr_(4k,5k)(11.05k,11.25k)\", (), 2584, predict_dict, \"MLPRegressorModel\").show()\n",
    "plot_predictions(\"gBOTH_raw_normalized_partfltr_(4k,5k)(11.05k,11.25k)\", (4000,5000), 2584, predict_dict, \"MLPRegressorModel\", PCA_transform_on=\"gBOTH_raw_normalized_partfltr_(4k,5k)(11.05k,11.25k)\").show()\n",
    "plot_predictions(\"gBOTH_raw_normalized_partfltr_(4k,5k)(11.05k,11.25k)\", (11050,11250), 2584, predict_dict, \"MLPRegressorModel\", PCA_transform_on=\"gBOTH_raw_normalized_partfltr_(4k,5k)(11.05k,11.25k)\").show()\n",
    "\n",
    "# Alright predictions, but need to be modified for energy:\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2402, unnormalized_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.00809, 9.5)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2406, unnormalized_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.00864, 8.8455)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2411, unnormalized_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.00614, 7.5068)).show() # <- 10.03\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2388, unnormalized_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.00059, 0.7411)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2392, unnormalized_dict, \"MLPRegressorModel\", custom_func=part_lin(0.00320, -0.8021)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2391, unnormalized_dict, \"MLPRegressorModel\", custom_func=part_lin(0.00546, -2.78)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2395, unnormalized_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.00392, 4.1411)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2398, unnormalized_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.00195)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2424, unnormalized_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.01268, 6)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2430, unnormalized_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.00536)).show() # <- 11.3153\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2440, combined_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.00580)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2447, combined_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.00260)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2464, combined_dict, \"MLPRegressorModel\", custom_func=part_lin(-0.00497)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2398, unnormalized_dict, \"MLPRegressorModel\", part_lin(0,0)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 1, combined_dict, \"XGBoostedTree\", part_lin(0,0)).show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2, unnormalized_dict, \"TF_NN_22-16-1_relu_0.0001\").show()\n",
    "# predict_from_model(exp_data_name, \"ALL\", 2, unnormalized_dict, \"TF_NN_22-16-1_relu_0.0001\", custom_func=part_lin(-0.01646, 19)).show() # <- 11.9124\n",
    "# regr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5476fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTIMATION OF FWHM AFTER 511, 1274 OVERLAPPING\n",
    "\n",
    "# @np.vectorize\n",
    "# def f_surf(x: np.ndarray|float, y: np.ndarray|float, root: float, multiple: float, constant: float = 0):\n",
    "#     return multiple * ( (x**root+y**root)**(1/root) ) + constant\n",
    "\n",
    "# list_index = [2406, 2402, 2388, 2411, 2392, 2391, 2395, 2398, 2424, 2430, 2440, 2447, 2464]\n",
    "# list_511 = [14.62, 18.13, 17.63, 15.01, 17.31, 13.45, 16.39, 15.17, 15.58, 14.62, 13.96, 16.06, 13.83]\n",
    "# list_1274 = [8.36, 8.21, 8.14, 7.47, 11.84, 11.64, 19.12, 9.11, 36.43, 9.02, 8.48, 7.82, 9.42]\n",
    "# list_FWHM = [10.83, 11.53, 11.41, 10.03, 13.84, 12.40, 16.90, 11.79, 22.35, 11.32, 10.87, 10.90, 11.31]\n",
    "\n",
    "# df_cross_model = pd.DataFrame({\"511\":list_511,\"1274\":list_1274,\"combined\":list_FWHM},index=pd.Series(list_index, name=\"version\")).rename_axis(\"FWHM\",axis=1)\n",
    "\n",
    "# # df_cross_model[\"pred[0.7,0.35]\"] = f_surf(df_cross_model[\"511\"], df_cross_model[\"1274\"], 0.7, 0.35)\n",
    "# # print(((df_cross_model[\"pred[0.7,0.35]\"] - df_cross_model[\"combined\"])**2).sum())\n",
    "# # df_cross_model[\"pred[0.6,0.3]\"] = f_surf(df_cross_model[\"511\"], df_cross_model[\"1274\"], 0.6, 0.3)\n",
    "# # print(((df_cross_model[\"pred[0.6,0.3]\"] - df_cross_model[\"combined\"])**2).sum())\n",
    "# try_0 = 0.4, 0.15, 1.38\n",
    "# try_1 = 0.3, 0.09, 0.65\n",
    "# try_2 = 0.25, 0.055, 1.03\n",
    "# for try_ in [try_0, try_1, try_2]:\n",
    "#     df_cross_model[f\"pred{try_}\"] = f_surf(df_cross_model[\"511\"], df_cross_model[\"1274\"], *try_)\n",
    "#     print(((df_cross_model[f\"pred{try_}\"] - df_cross_model[\"combined\"])**2).sum())\n",
    "\n",
    "# display(df_cross_model)\n",
    "# xx, yy = np.linspace(10, 20, 21), np.linspace(7, 37, 31)\n",
    "# x,y = np.meshgrid(xx, yy)\n",
    "# z_0 = f_surf(x, y, *try_0) #0.4, 0.18, -1\n",
    "# z_1 = f_surf(x, y, *try_1)\n",
    "# z_2 = f_surf(x, y, *try_2)\n",
    "# # zz = np.linspace(z.min(), z.max(), 100)\n",
    "\n",
    "# fig = go.Figure(data=[go.Surface(z=z_1, x=x, y=y)])\n",
    "# fig.add_trace(go.Surface(z=z_2, x=x, y=y))\n",
    "# fig.add_trace(go.Scatter3d(x=df_cross_model[\"511\"],y=df_cross_model[\"1274\"],z=df_cross_model[\"combined\"]))\n",
    "# # display(df_cross_model[f\"pred{try_2}\"] - df_cross_model[\"combined\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d2f0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_for_regressor.iloc[0, :] = labels_t\n",
    "# display(data_for_regressor)\n",
    "# data_to_predict = PCA(64).fit_transform(data_for_regressor.values.transpose())\n",
    "#  # Combining labels and creating prediction Series\n",
    "# s_labels_t = pd.Series(labels_t)\n",
    "# shift = -round(s_labels_t.mean())\n",
    "# s_labels_t.name = f\"Initial data: dT {'-' if shift < 0 else '+'} {abs(shift)} ns\"\n",
    "# predicted_x = regressor.predict(data_to_predict)\n",
    "# pred_s = pd.Series(labels_t - x_to_t(predicted_x),name=\"dT_act - dT_pred\")\n",
    "\n",
    "# # Add histogram with predicted vs actual data\n",
    "# fig_hist = histogram(pd.concat([s_labels_t+shift,pred_s], axis=1), [-30,30,0.25], title=\"Arrival Time Histogram\", xaxis_title=\"Time (ns)\", yaxis_title=\"Prevalence\")\n",
    "# fig_hist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single experiment\n",
    "model_type = \"sklearn\" # or \"sklearn\" or \"xgboost\"\n",
    "                                            # data511 or datag511partfltr or datag511unfiltered\n",
    "data_and_name = datag511partfltr,data_g511_partfltr_name # data_g511_name or data_g511_partfltr_name\n",
    "data_and_name = select_from_source(datag511partfltr_all,select_channels=[0,2,5,8,9,10]),name_g511_d([0,2,5,8,9,10]) # test: 1,3,7,11\n",
    "# data_temp_dict = {data_and_name[0]:data_and_name[1]}\n",
    "\n",
    "if model_type == \"xgboost\":\n",
    "    result_single_exp = run_experiment(\n",
    "        data=data_and_name[0],  \n",
    "        data_name=data_and_name[1],\n",
    "        select_channels=[0],\n",
    "        select_energies=(11050,11250),\n",
    "        pca_components=None,\n",
    "        model_type=\"xgboost\",\n",
    "        max_depth=50,\n",
    "        n_estimators=3,\n",
    "        max_leaves= 0,\n",
    "        test_size=0.2,\n",
    "        uniform_test_set=[5000,6000,7000,8000,9000,10000,11000,12000]#,8000,9000,10000,11000]\n",
    "    )\n",
    "else:\n",
    "    result_single_exp = run_experiment(\n",
    "        data=data_and_name[0],\n",
    "        data_name=data_and_name[1],\n",
    "        select_channels=[],\n",
    "        select_energies=(5000,13000),\n",
    "        pca_components=None,\n",
    "        model_type=\"sklearn\",\n",
    "        hidden_layers=[23],\n",
    "        test_size=0.3,\n",
    "        uniform_test_set=[5000,6000,7000,8000,9000,10000,11000,12000], #,8000,9000,10000,11000]\n",
    "        dT_correcting=True\n",
    "    )#._params\n",
    "result_single_exp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561627ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data |= {\"partfilter_g511_all_E5000-13000\": select_from_source(datag511partfltr_all,select_channels=[0,2,5,8,9,10],select_energies=(5000,13000))}\n",
    "# display(datag511partfltr_all)\n",
    "check_ch = 7\n",
    "plot_predictions(name_g511_d(check_ch), (11000,11500), 2994, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_all_E5000-13000\").show()\n",
    "plot_predictions(name_g511_d(check_ch), (11000,11500), 2995, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_all_E5000-13000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfe491",
   "metadata": {},
   "source": [
    "### Run experiments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params= {\"model_type\": \"XGBoost\",\n",
    "                 \"select_channels\": [0,23],\n",
    "                #  \"max_iterations\": 2_000,\n",
    "                 \"remove_nan_waveforms\":True,\n",
    "                 \"select_energies\":(10300,12000), # DEFAULT for th06-th60: (11050,11250)\n",
    "                 \"include_energy\": False,\n",
    "                 \"max_leaves\": 0,\n",
    "                 \"pca_method\": PCA,\n",
    "                 \"uniform_test_set\": [5000,6000,7000,8000,9000,10000,11000,12000],\n",
    "                 \"test_size\":0.3}\n",
    "\n",
    "# data = {\"gBOTH_raw_normalized_partfltr_(4k,5k)(11.05k,11.25k)\": data_dict[\"raw\"][\"normalized\"][\"FIR\"][\"gBOTH\"][\"range_cut\"]} #{data_g511_unfiltered_name:datag511unfiltered} # {data_g1274_name: data1274} #{exp_data_unfiltered_name: exp_data_unfiltered}\n",
    "data = {data_g511_partfltr_name:datag511partfltr}\n",
    "\n",
    "#             data_g1274_partfltr_name_all:datag1274partfltr_all,\n",
    "            #  data_g511_partfltr_name_all:datag511partfltr_all,\n",
    "pca_components_list = [None] #[16,23,100,None]\n",
    "estimators = [2,4] #[1,2,4]\n",
    "depth = [40,50]\n",
    "select_energies = [(8000,50000),(8000,13000)]\n",
    "exp_list = [[data_name,pca_comp,trees,depth,energies] for data_name,pca_comp,trees,depth,energies in itertools.product([k for k in data.keys()],pca_components_list,estimators,depth,select_energies)]\n",
    "print(exp_list)\n",
    "iterations = [4]*len(exp_list)\n",
    "print(len(exp_list),\"experiments,\",sum(iterations), f\"iterations (={sum(iterations)/len(exp_list)}*{len(pca_components_list)}*{len(estimators)}*{len(depth)}*{len(select_energies)})\")\n",
    "results = {}\n",
    "# break\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:30000\")\n",
    "for exp,iters in zip(exp_list,iterations):\n",
    "    params = default_params\n",
    "    params |= {\"pca_components\": exp[1], \"n_estimators\": exp[2], \"max_depth\":exp[3], \"select_energies\": exp[4]}\n",
    "    result = {str(i):None for i in range(iters)}\n",
    "    if [exp[1]] == exp[2]:\n",
    "        print(f\"Got layers {[exp[1]]} equal to {exp[2]}, skipping experiment\")\n",
    "        continue\n",
    "    for i in range(iters):\n",
    "        print(f\"[MLFlow run] Starting iteration {i+1}/{iters} with params {params}...\")\n",
    "        if \"raw\" in exp[0]:\n",
    "            result[str(i)] = run_experiment(data[exp[0]], exp[0], **params)._params\n",
    "        else:\n",
    "            result[str(i)] = run_experiment(data_dict[exp[0]], exp[0], **params)._params\n",
    "    results[str(exp)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de984a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params= {\"model_type\": \"SKlearn\",\n",
    "                 \"select_channels\": [0,2,5,8,9,10],\n",
    "                 \"max_iterations\": 2_000,\n",
    "                 \"remove_nan_waveforms\":True,\n",
    "                #  \"select_energies\":(9000,13000), # DEFAULT for th06-th60: (11050,11250)\n",
    "                 \"include_energy\": False,\n",
    "                 \"activation\": 'relu',\n",
    "                 \"pca_method\": PCA,\n",
    "                 \"uniform_test_set\": [5000,6000,7000,8000,9000,10000,11000,12000],\n",
    "                 \"test_size\":0.3,\n",
    "                 \"dT_correcting\": True}\n",
    "\n",
    "# data = {\"gBOTH_raw_normalized_partfltr_(4k,5k)(11.05k,11.25k)\": data_dict[\"raw\"][\"normalized\"][\"partfltr\"][\"gBOTH\"][\"range_cut\"]} #{data_g511_unfiltered_name:datag511unfiltered} # {data_g1274_name: data1274} #{exp_data_unfiltered_name: exp_data_unfiltered}\n",
    "# data = {data_g511_partfltr_name:datag511partfltr}\n",
    "data_run = {name_g511_d([0,2,5,8,9,10]):select_from_source(datag511partfltr_all,select_channels=[0,2,5,8,9,10])}\n",
    "\n",
    "pca_components_list = [None] #[100,None] #[16,18,20,21,22,23,64]\n",
    "hidden_layers_list = [[100]] #[5,10,20,50]\n",
    "select_energies = [(5000,13000)] #[(9000,50000),(8000,50000)] #[(10000,50000),(5000,13000),()]\n",
    "exp_list = [[obj for obj in tup] for tup in itertools.product([k for k in data_run.keys()],pca_components_list,hidden_layers_list,select_energies)]\n",
    "print(exp_list)\n",
    "iterations = [1]*len(exp_list)\n",
    "print(len(exp_list),\"experiments,\",sum(iterations), f\"iterations (={sum(iterations)/len(exp_list)}*{len(pca_components_list)}*{len(hidden_layers_list)})\")\n",
    "results = {}\n",
    "# break\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\") # 5000: local, 30000: external\n",
    "iteration = 0\n",
    "for exp,iters in zip(exp_list,iterations):\n",
    "    params = default_params\n",
    "    params |= {\"pca_components\": exp[1], \"hidden_layers\": exp[2], \"select_energies\": exp[3]}\n",
    "    result = {str(i):None for i in range(iters)}\n",
    "    # if [exp[1]] == exp[2] == 22:\n",
    "    #     print(f\"Got layers {[exp[1]]} equal to {exp[2]}, skipping experiment\")\n",
    "    #     continue\n",
    "    for i in range(iters):\n",
    "        iteration += 1\n",
    "        print(f\"[MLFlow run] Starting iteration {i+1}/{iters} ({iteration}/{sum(iterations)}) with params {params}...\")\n",
    "        if \"raw\" in exp[0]:\n",
    "            result[str(i)] = run_experiment(data_run[exp[0]], exp[0], **params)._params\n",
    "        else:\n",
    "            result[str(i)] = run_experiment(data_run[exp[0]], exp[0], **params)._params\n",
    "    results[str(exp)] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c8af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "# {\"SKLearnNN, Na22 Ch0\":918309536924112984}\n",
    "data = {data_g511_partfltr_name:datag511partfltr,\n",
    "        data_g511_partfltr_name_all:datag511partfltr_all,\n",
    "        # \"partfilter_g511_E8000-50000\":select_from_source(datag511partfltr,select_energies=(8000,50000)),\n",
    "        # \"partfilter_g511_E5000-13000\":select_from_source(datag511partfltr,select_energies=(5000,13000)),\n",
    "        # \"partfilter_g511_E8000-13000\":select_from_source(datag511partfltr,select_energies=(8000,13000)),\n",
    "        }\n",
    "# get_waveforms(source_data=data, select_energies=range_E, include_energy=False)\n",
    "# Best results:         abs                                 avg\n",
    "# E5000-6000    13.358  8000-50000, 100-[23] @ 10.485       5000-13000, 100-[16] @ 10.71\n",
    "# E6000-7000    11.841  9000-13000, 23-[100] @  9.227       9000-13000, 23-[100] @  9.61\n",
    "# E7000-8000    10.374  9000-50000, 23-[23]  @  8.682       9000-50000, 23-[100] @  8.82\n",
    "# E8000-9000    10.292  5000-13000, ALL-[100] @ 8.371       5000-13000, ALL-[100] @ 8.49\n",
    "# E9000-10000    9.439  9000-13000, 23-[100] @  7.689       5000-13000, 100-[16] @  7.92\n",
    "# E10000-11000  11.282  5000-13000, ALL-[100] @ 7.034       5000-13000, ALL-[100] @ 7.12\n",
    "# E11000-12000   7.064  5000-13000, 100-[16] @  6.004       5000-13000, 100-[16] @  6.28\n",
    "\n",
    "# Eleastsquares 2949:  8000-50000 100-[23]  @ 5.599, 2897:  5000-13000 ALL-[100] @ 5.608, 2851:  5000-13000 100-[16] @ 5.605\n",
    "#                739:  8000-13000 16-4*40   @ 3.444,  748:  8000-13000 16-4*50   @ 3.475,  743:  8000-50000 16-4*50  @ 3.512\n",
    "# Elinear       2897:  5000-13000 ALL-[100] @ 5.651, 2949:  8000-50000 100-[23]  @ 5.654, 2851:  5000-13000 100-[16] @ 5.662\n",
    "#                748:  8000-13000 16-4*50   @ 4.367,  743:  8000-50000 16-4*50   @ 4.410,  739:  8000-13000 16-4*40  @ 4.447\n",
    "\n",
    "\n",
    "ignore_values = {\"Energy range used\": \"() eV\"}\n",
    "ignore_values1 = {\"tree depth\": None}\n",
    "energy = 5000\n",
    "y = f\"FWHM E{energy}-{energy+1000}\"\n",
    "\n",
    "def square_FWHM_metric(df: pd.DataFrame) -> pd.Series:\n",
    "    ranges = {\"5000-6000\":13.3579,\"6000-7000\":11.841,\"7000-8000\":10.3735,\"8000-9000\":10.2916,\"9000-10000\":9.4387,\"10000-11000\":11.2823,\"11000-12000\":7.0638}\n",
    "    return sum([(df[f\"metrics.Uniform test FWHM E{e_range}\"]/e_val)**2 for e_range,e_val in ranges.items()])\n",
    "\n",
    "def linear_FWHM_metric(df: pd.DataFrame) -> pd.Series:\n",
    "    ranges = {\"5000-6000\":13.3579,\"6000-7000\":11.841,\"7000-8000\":10.3735,\"8000-9000\":10.2916,\"9000-10000\":9.4387,\"10000-11000\":11.2823,\"11000-12000\":7.0638}\n",
    "    return sum([df[f\"metrics.Uniform test FWHM E{e_range}\"]/e_val for e_range,e_val in ranges.items()])\n",
    "linear_FWHM_metric.FWHM = square_FWHM_metric.FWHM = 7\n",
    "\n",
    "y = square_FWHM_metric\n",
    "# df_version_mapper = get_model_version_map([102816600889877627])\n",
    "# display(df_version_mapper)\n",
    "# display(get_model_version_map([918309536924112984]))\n",
    "boxplot([918309536924112984], x=\"Energy range used\", y=y, color=\"PCA components\", ignore_vals=ignore_values, facet_row=\"Hidden layers\", height=700, hover_name=\"model_version\").show()\n",
    "boxplot([102816600889877627], x=\"estimators\", y=y, color=\"PCA components\", ignore_vals=ignore_values1, facet_row=\"tree depth\", facet_col=\"Energy range used\", hover_name=\"model_version\", height=800).show()\n",
    "boxplot([941575026271596123], x=\"PCA components\", y=y, color=\"Hidden layers\", height=700, hover_name=\"model_version\").show()\n",
    "plot_predictions(data_g511_partfltr_name, (), 3032, data, \"MLPRegressorModel\", PCA_fit=fitted_PCA(3032, waveforms)).show()\n",
    "plot_predictions(data_g511_partfltr_name, (), 3028, data, \"MLPRegressorModel\", PCA_fit=fitted_PCA(3028, waveforms)).show()\n",
    "plot_predictions(data_g511_partfltr_name, (), 3000, data, \"MLPRegressorModel\", PCA_fit=fitted_PCA(3000, waveforms)).show()\n",
    "plot_predictions(data_g511_partfltr_name, (), 3033, data, \"MLPRegressorModel\").show()\n",
    "# plot_predictions(data_g511_partfltr_name, (), 2949, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E8000-50000\").show()\n",
    "# plot_predictions(data_g511_partfltr_name, (), 2897, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E5000-13000\").show()\n",
    "# plot_predictions(data_g511_partfltr_name, (), 2851, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E5000-13000\").show()\n",
    "# plot_predictions(data_g511_partfltr_name, (), 739, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-13000\").show()\n",
    "# plot_predictions(data_g511_partfltr_name, (), 748, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-13000\").show()\n",
    "# plot_predictions(data_g511_partfltr_name, (), 743, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-50000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51f1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on other detectors\n",
    "detector = 1\n",
    "data |= {name_g511_d(detector): data_dict[name_g511_d(detector)]}\n",
    "# predict_from_model(name_g511_d(detector), (), 2949, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E8000-50000\").show()\n",
    "# predict_from_model(name_g511_d(detector), (), 2897, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E5000-13000\").show()\n",
    "# predict_from_model(name_g511_d(detector), (), 2851, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E5000-13000\").show()\n",
    "# predict_from_model(name_g511_d(detector), (), 739, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-13000\").show()\n",
    "# predict_from_model(name_g511_d(detector), (), 748, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-13000\").show()\n",
    "# predict_from_model(name_g511_d(detector), (), 743, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-50000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c84d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on other detectors\n",
    "detector = 2\n",
    "data |= {name_g511_d(detector): data_dict[name_g511_d(detector)]}\n",
    "predict_from_model(name_g511_d(detector), (), 2949, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E8000-50000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(detector), (), 2897, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E5000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(detector), (), 2851, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E5000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(detector), (), 739, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(detector), (), 748, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(detector), (), 743, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-50000\", xaxis_range=[-30,120]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on other detectors\n",
    "detector = 3\n",
    "data |= {name_g511_d(detector): data_dict[name_g511_d(detector)]}\n",
    "predict_from_model(name_g511_d(detector), (), 2949, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E8000-50000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(detector), (), 2897, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E5000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(detector), (), 2851, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E5000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(detector), (), 739, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(detector), (), 748, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(detector), (), 743, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-50000\", xaxis_range=[-30,120]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53faac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on part of set of detector 3\n",
    "predict_from_model(name_g511_d(3), (5000,50000), 2949, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E8000-50000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(3), (5000,50000), 2897, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E5000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(3), (5000,50000), 2851, data, \"MLPRegressorModel\", PCA_transform_on=\"partfilter_g511_E5000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(3), (5000,50000), 739, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(3), (5000,50000), 748, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-13000\", xaxis_range=[-30,120]).show()\n",
    "predict_from_model(name_g511_d(3), (5000,50000), 743, data, \"XGBoostedTree\", PCA_transform_on=\"partfilter_g511_E8000-50000\", xaxis_range=[-30,120]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca97990",
   "metadata": {},
   "source": [
    "### Visualise experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5f7c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_sigma_lists = lambda dic: {k:[{d_k:pd.Series([d[str(i)][d_k][\"sigma\"] for i in range(len(d))])} for d_k in d[\"0\"].keys() if not \"data\" in d_k] for k,d in dic.items()}\n",
    "exp_results = lambda dic: {k:{\"test\" if \"test\" in d_k else \"train\": ls_d for d_k,ls_d in (ls[0]|ls[1]).items()} for k,ls in dic.items()}\n",
    "exp_res = lambda dic: {key: {k[k.find(\", \")+2: k.find(\"[\",k.find(\", \"))-2]:v for k,v in dic.items() if key in k} for key in {ky[ky.rfind(\"[\"):ky.find(\"]\")+1] for ky in dic.keys()}}\n",
    "series_ls = lambda dic: [pd.Series(s, name=n) for n,s in {k+\"_\"+i+\"_\"+j: v[i][j] for k,v in dic.items() for j in [\"test\",\"train\"] for i in v.keys()}.items()]\n",
    "df_results = pd.DataFrame(series_ls(exp_res(exp_results(exp_sigma_lists(results)))))\n",
    "# df_results.index = pd.Index([\"[\"+i[i.rfind(\"'\")+3:i.rfind(\"]\")+1]+i[i.rfind(\"_\"):] for i in df_results.index])\n",
    "display(df_results)\n",
    "print(df_results.index)\n",
    "# df_results.to_csv(\"data/results/NN_TruncSVC_testing_gBOTHpartfltr_4000-11250.csv\")\n",
    "mult = 2*np.sqrt(2*np.log(2))\n",
    "# break\n",
    "df = pd.read_csv(\"data/results/XGBoost_g511unfiltered_10350-14000.csv\").set_index(\"experiment\")\n",
    "df_new = pd.DataFrame(data=[(i[1:i.find(\",\")],i[i.find(\",\")+1:i.rfind(\",\")], i[i.rfind(\",\")+1:i.find(\"]\")], \"test\" if \"test\" in i else \"train\", val*mult) for i,s in df.iterrows() for val in s.values if not pd.isna(val)],\n",
    "                      columns=[\"PCA components\",\"# estimators\",\"Max tree depth\",\"set\",\"FWHM\"])\n",
    "# df_new\n",
    "px.box(df_new, x=\"PCA components\", y=\"FWHM\", color=\"# estimators\",facet_col=\"set\",\n",
    "       hover_data=[\"set\",\"# estimators\"], points=\"all\" # add day column to hover data\n",
    "       ).update_traces(boxmean=True).update_layout(title=\"Box plots of XGBoost hyperparameter optimization (g511 & g1274 unfiltered)\").show()\n",
    "\n",
    "df = pd.read_csv(\"data/results/XGBoost2_g511unfiltered_10350-14000.csv\").set_index(\"experiment\")\n",
    "df_new = pd.DataFrame(data=[(i[1:i.find(\",\")],i[i.find(\",\")+1:i.rfind(\",\")], i[i.rfind(\",\")+1:i.find(\"]\")], \"test\" if \"test\" in i else \"train\", val*mult) for i,s in df.iterrows() for val in s.values if not pd.isna(val)],\n",
    "                      columns=[\"PCA components\",\"# estimators\",\"Max tree depth\",\"set\",\"FWHM\"])\n",
    "px.box(df_new, x=\"Max tree depth\", y=\"FWHM\", color=\"# estimators\",facet_col=\"set\", facet_row=\"PCA components\",\n",
    "       hover_data=[\"set\",\"# estimators\"], points=\"all\" # add day column to hover data\n",
    "       ).update_traces(boxmean=True).update_layout(title=\"Box plots of XGBoost hyperparameter optimization (g511unfiltered, 10350<E<14000)\", height=750).show()\n",
    "\n",
    "df = pd.read_csv(\"data/results/PCA_testing_g1274_4000-5000.csv\").set_index(\"experiment\")\n",
    "df_new = pd.DataFrame(data=[(i[:i.find(\"_\")], i[i.find(\"_\")+1:i.rfind(\"_\")], \"test\" if \"test\" in i else \"train\", val*mult) for i,s in df.iterrows() for val in s.values if not pd.isna(val)],\n",
    "                      columns=[\"Hidden layers\",\"PCA components\",\"set\",\"FWHM\"])\n",
    "px.box(df_new[df_new[\"set\"] == \"test\"], x=\"PCA components\", y=\"FWHM\", color=\"Hidden layers\", #[df_new[\"set\"] == \"test\"]\n",
    "       hover_data=[\"set\"], points=\"all\" # add day column to hover data\n",
    "       ).update_traces(boxmean=True).update_layout(title=\"Box plots of PCA analysis (g1274, 4000<E<5000)\").show()\n",
    "\n",
    "df = pd.read_csv(\"data/results/PCA_testing_g511_10350-14000.csv\").set_index(\"experiment\")\n",
    "df_new = pd.DataFrame(data=[(i[:i.find(\"_\")], i[i.find(\"_\")+1:i.rfind(\"_\")], \"test\" if \"test\" in i else \"train\", val*mult) for i,s in df.iterrows() for val in s.values if not pd.isna(val)],\n",
    "                      columns=[\"Hidden layers\",\"PCA components\",\"set\",\"FWHM\"])\n",
    "px.box(df_new[df_new[\"set\"] == \"test\"], x=\"PCA components\", y=\"FWHM\", color=\"Hidden layers\",\n",
    "       hover_data=[\"set\"], points=\"all\" # add day column to hover data\n",
    "       ).update_traces(boxmean=True).update_layout(title=\"Box plots of PCA analysis (g511, 10350<E<14000)\").show()\n",
    "\n",
    "df = pd.read_csv(\"data/results/PCA_testing_g511_11050-11250.csv\").set_index(\"experiment\")\n",
    "df_new = pd.DataFrame(data=[(i[:i.find(\"_\")], i[i.find(\"_\")+1:i.rfind(\"_\")], \"test\" if \"test\" in i else \"train\", val*mult) for i,s in df.iterrows() for val in s.values if not pd.isna(val)],\n",
    "                      columns=[\"Hidden layers\",\"PCA components\",\"set\",\"FWHM\"])\n",
    "px.box(df_new[df_new[\"set\"] == \"test\"], x=\"PCA components\", y=\"FWHM\", color=\"Hidden layers\", #[df_new[\"set\"] == \"test\"]\n",
    "       hover_data=[\"set\"], points=\"all\" # add day column to hover data\n",
    "       ).update_traces(boxmean=True).update_layout(title=\"Box plots of PCA analysis (g511, 11050<E<11250)\").show()\n",
    "\n",
    "df = pd.read_csv(\"data/results/PCA_testing_g511unfiltered_11050-11250.csv\").set_index(\"experiment\")\n",
    "df_new = pd.DataFrame(data=[(i[:i.find(\"_\")], i[i.find(\"_\")+1:i.rfind(\"_\")], \"test\" if \"test\" in i else \"train\", val*mult) for i,s in df.iterrows() for val in s.values if not pd.isna(val)],\n",
    "                      columns=[\"Hidden layers\",\"PCA components\",\"set\",\"FWHM\"])\n",
    "px.box(df_new[df_new[\"set\"] == \"test\"], x=\"PCA components\", y=\"FWHM\", color=\"Hidden layers\", #[df_new[\"set\"] == \"test\"]\n",
    "       hover_data=[\"set\"], points=\"all\" # add day column to hover data\n",
    "       ).update_traces(boxmean=True).update_layout(title=\"Box plots of PCA analysis (g511 unfiltered, 11050<E<11250)\").show()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/results/PCA_testing_2-128_th60.csv\").set_index(\"experiment\")\n",
    "df_new = pd.DataFrame(data=[(i[:i.find(\"_\")], i[i.find(\"_\")+1:i.rfind(\"_\")], \"test\" if \"test\" in i else \"train\", val*mult) for i,s in df.iterrows() for val in s.values if not pd.isna(val)],\n",
    "                      columns=[\"Hidden layers\",\"PCA components\",\"set\",\"FWHM\"])\n",
    "px.box(df_new[df_new[\"set\"] == \"test\"], x=\"PCA components\", y=\"FWHM\", color=\"Hidden layers\",\n",
    "       hover_data=[\"set\"], points=\"all\" # add day column to hover data\n",
    "       ).update_traces(boxmean=True).update_layout(title=\"Box plots of PCA analysis (th60)\").show()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/results/PCA_testing_gBOTH_4000-11250.csv\").set_index(\"experiment\")\n",
    "df_new = pd.DataFrame(data=[(i[:i.find(\"_\")], i[i.find(\"_\")+1:i.rfind(\"_\")], \"test\" if \"test\" in i else \"train\", val*mult) for i,s in df.iterrows() for val in s.values if not pd.isna(val)],\n",
    "                      columns=[\"Hidden layers\",\"PCA components\",\"set\",\"sigma\"])\n",
    "px.box(df_new[df_new[\"set\"] == \"test\"], x=\"PCA components\", y=\"sigma\", color=\"Hidden layers\",\n",
    "       hover_data=[\"set\"], points=\"all\" # add day column to hover data\n",
    "       ).update_traces(boxmean=True).update_layout(title=\"Box plots of PCA analysis g511 & g1274 COMBINED (test)\", yaxis_title=\"FWHM\").show()\n",
    "px.box(df_new[df_new[\"set\"] == \"train\"], x=\"PCA components\", y=\"sigma\", color=\"Hidden layers\",\n",
    "       hover_data=[\"set\"], points=\"all\" # add day column to hover data\n",
    "       ).update_traces(boxmean=True).update_layout(title=\"Box plots of PCA analysis g511 & g1274 COMBINED (train)\", yaxis_title=\"FWHM\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ead76",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_final_max1 = data_df[-10:].mean() < 1.011\n",
    "condition_final_max2 = (data_df[-10:].mean() < 1.011)\n",
    "# display(data_df)\n",
    "display(data_df.loc[:,condition_final_max2].shape)\n",
    "print(data_df.values.transpose()[condition_final_max1].shape)\n",
    "labels_t = np.array([float([s[s.find(\"]\")+1:s.find(\"dT\")] for s in [col.replace(\" \",\"\")]][0]) for col in data_df.columns])\n",
    "labels_x = t_to_x(labels_t)\n",
    "wave_i = np.array([int(col[col.find(\"[\")+1:col.find(\"]\")]) for col in data_df.columns])\n",
    "# wave_i_map\n",
    "train_test_list = d_train, d_test, wave_i_train, wave_i_test, l_t_train, l_t_test = train_test_split_cond(data_df.values.transpose(), wave_i, labels_t, test_size=0.5, random_state=42, condition=condition_final_max1, add_removed_to_test=True)\n",
    "df_train=pd.DataFrame(data=d_train.T, columns = wave_i_train)\n",
    "display(df_train)\n",
    "\n",
    "print(data_df.shape)\n",
    "print(str([item.shape for item in train_test_list]).strip(\"[]\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf4d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = [18527] #4066, 13873, 15418, 5073, 6122, 15841, 9151, 18527\n",
    "wave_df, imap = get_waveforms(waveform, source_data=data06, get_indices_map=True)\n",
    "col_for_str = lambda df, col: [c for c in df.columns if col in c][0]\n",
    "\n",
    "df_new, df_res = df_with_fits(wave_df, 0.2, return_results=True, exponent_fit=True, y_min=0.7)\n",
    "display(df_res)\n",
    "x0, xT, x_cutoff = df_res.loc[str(waveform)][\"x0\"], df_res.loc[str(waveform)][\"xT\"], df_res.loc[str(waveform)][\"x_cutoff\"]\n",
    "fig2 = df_new.iplot(title=f\"Plot of waveform {waveform} with fitted parabola (start-end: {x0:.2f}-{x_cutoff:.0f}, xT @ {xT:.3f})\",asFigure=1)\n",
    "fig2.add_vline(x=xT, line_color=\"orange\").show()\n",
    "display(HTML(\"<br>\".join([f\"<b>{k}</b>: {v}\" for k,v in {\"a\":\"parabola constant\",\"x0\":\"peak of parabola\",\"x_cutoff\":\"end of parabola\",\"var\":\"parabola and line fit variance\",\"x0_cutoff_var\":\"variance from x0 to x_cutoff (parabola range)\",\"xT\":\"x position of dT\",\"yT_fit\":\"y position of dT according to fitted parabola\",\"final10_slope\":\"Slope of the final 10 data points\",\"b\":\"Final exponential fit base parameter\",\"c\":\"x value where exp. fit is zero\",\"exp_start\":\"Start of the exponential fit\",\"exp_var\":\"Variance of the exp. fit\"}.items()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede6663-317b-4d8e-bf8a-95144a8fef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "new_max_col = 20000\n",
    "df = get_waveforms(0, len(data06), source_data=data06) #len(data0)\n",
    "\n",
    "RESULTS_FROM_CACHE = True\n",
    "\n",
    "print(datetime.now() - start, \"elapsed while getting waveforms\")\n",
    "if RESULTS_FROM_CACHE is False:\n",
    "    df_new, df_results = df_with_fits(df, 0.2, return_results=True, force_max_col=new_max_col, exponent_fit=True, y_min=0.7)\n",
    "    print(datetime.now() - start, \"elapsed while getting waveforms & fitting parabolas\")\n",
    "else:\n",
    "    df_results = pd.read_parquet(\"results_df_with_fits_exp.parquet\")\n",
    "    print(datetime.now() - start, \"elapsed while getting waveforms fit parabolas results\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "recalc_low_acc_results = fit_parabolas(df[[col for col in df.columns if sum([i in col for i in df_results[df_results[\"x0_cutoff_var\"] > 0.00002].index]) > 0]], 0.1, force_max_col=new_max_col)\n",
    "end = time.perf_counter()\n",
    "print(f\"Elapsed for recalculating inaccurate ones (after compilation) = {end - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f18637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results.to_parquet(\"results_df_with_fits_exp_big_set.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0713607-46e4-401c-9b3d-1101d1e2b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_acc_results = df_results[df_results[\"x0_cutoff_var\"] > 0.00002]\n",
    "past_cutoff_results = df_results[df_results[\"x_cutoff\"] < df_results[\"xT\"]]\n",
    "# slope_results = df_results[df_results[\"final10_slope\"] > 0.013]\n",
    "notable_results = pd.concat([low_acc_results, past_cutoff_results],axis=0)\n",
    "# Replace the low accuracy results with the new 'recalc_low_acc_results' values\n",
    "updated_results = pd.concat([df_results[df_results[\"x0_cutoff_var\"] <= 0.00002], recalc_low_acc_results],axis=0)\n",
    "\n",
    "# display(df_results[\"[190]\":\"[191]\"])\n",
    "# display(recal_low_acc_results)\n",
    "# display(low_acc_results)\n",
    "# display(parabola_results)\n",
    "# df_new[[col for col in df_new.columns if sum([i in col for i in notable_results.index]+[\"[190]\" in col]) >= 1]].iplot()\n",
    "kwargs = {\"yaxis_range\":[-23,35], \"xaxis_range\":[0, .0014]}\n",
    "\n",
    "fig0 = corr_fig(df_results, [], (None, 2e-5), **kwargs)\n",
    "popt0, pcov0 = curve_fit(lambda x, m, b: m*x + b, df_results[\"a\"], df_results[\"xT\"]-df_results[\"x0\"], bounds=([-10000, 1], [10, 20]))\n",
    "print(\"df_results:\", popt0, pcov0, \"\\nx0,y0 - x1,y1:\", (0,popt0[1]), (0.001, popt0[0]/1000+popt0[1]))\n",
    "fig0.update_layout(shapes = [{'type': 'line', 'yref': 'y', 'xref': 'x', 'y0': popt0[1], 'y1': popt0[0]/1000+popt0[1], 'x0': 0, 'x1': 0.001}])\n",
    "# fig0.show()\n",
    "\n",
    "corr_fig(past_cutoff_results, **kwargs).update_layout(title=\"Past cutoff results\",shapes = [{'type': 'line', 'yref': 'y', 'xref': 'x', 'y0': popt0[1], 'y1': popt0[0]/1000+popt0[1], 'x0': 0, 'x1': 0.001}]).show()\n",
    "# corr_fig(slope_results, **kwargs).update_layout(title=\"Final slope results\",shapes = [{'type': 'line', 'yref': 'y', 'xref': 'x', 'y0': popt0[1], 'y1': popt0[0]/1000+popt0[1], 'x0': 0, 'x1': 0.001}]).show()\n",
    "# corr_fig(low_acc_results, **kwargs).update_layout(shapes = [{'type': 'line', 'yref': 'y', 'xref': 'x', 'y0': popt0[1], 'y1': popt0[0]/1000+popt0[1], 'x0': 0, 'x1': 0.001}]).show()\n",
    "\n",
    "# fig2 = corr_fig(updated_results, [], (None, 2e-5), **kwargs)\n",
    "# popt1, pcov1 = curve_fit(lambda x, m, b: m*x + b, updated_results[\"a\"], updated_results[\"xT\"]-updated_results[\"x0\"], bounds=([-10000, 1], [10, 20]))\n",
    "# print(\"updated_results:\", popt1, pcov1, \"\\nx0,y0 - x1,y1:\", (0,popt1[1]), (0.001, popt1[0]/1000+popt1[1]))\n",
    "# fig2.update_layout(shapes = [{'type': 'line', 'yref': 'y', 'xref': 'x', 'y0': popt1[1], 'y1': popt1[0]/1000+popt1[1], 'x0': 0, 'x1': 0.001}])\n",
    "# fig2.show() #  \"line_dash\":\"dash\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb211847-a126-4057-855a-7526e046a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[\"exp_x_99%\"] = np.log(100)/np.log(df_results[\"b\"]) + df_results[\"c\"]\n",
    "display(df_results)\n",
    "# print([] or None or {} or 0.0 or [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8767ef4-fae8-4100-83ba-ec850826cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to see training data structure\n",
    "used_data, data_name = data4, data4_name\n",
    "data_df = get_waveforms(0, len(used_data), source_data=used_data, get_indices_map=False)#[10:]\n",
    "display(data_df.head())\n",
    "data_df_last_10 = data_df[-10:].diff().mean().values #_fit_final_slope(data_df.values, data_df.columns).flatten()\n",
    "print(len(data_df_last_10),\"long array\",data_df_last_10)#,\"\\n\", _fit_final_slope(data_df.values, data_df.columns).flatten())\n",
    "data = data_df.values.transpose()\n",
    "labels_t = np.array([float([s[s.find(\"]\")+1:s.find(\"dT\")] for s in [col.replace(\" \",\"\")]][0]) for col in data_df.columns])\n",
    "labels_x = t_to_x(labels_t)\n",
    "wave_i = np.array([int(col[col.find(\"[\")+1:col.find(\"]\")]) for col in data_df.columns])\n",
    "\n",
    "cutoff_xT = -10\n",
    "cutoff_i_slope = 0.01\n",
    "\n",
    "condition_skimmed = labels_x > cutoff_xT\n",
    "condition_i_slope = (0 <= data_df[:11].diff().mean()) & (data_df[:11].diff().mean() <= cutoff_i_slope)\n",
    "condition_f40 = data_df[-40:].mean() < 1.125\n",
    "print(\"Skimmed condition length:\",sum(condition_skimmed))\n",
    "print(f\"Initial slope first 10 values <={cutoff_i_slope} avg. condition length:\",sum(condition_i_slope))\n",
    "\n",
    "available_conditions = {\n",
    "    f\"xT > {cutoff_xT}\": condition_skimmed,\n",
    "    f\"0 <= Initial slope <= {cutoff_i_slope}\": condition_i_slope,\n",
    "    f\"Final values < 1.125\": condition_f40}\n",
    "used_conditions = [1,2]\n",
    "\n",
    "applied_conditions = combine_and(*[list(available_conditions.values())[i] for i in used_conditions])\n",
    "applied_conditions_names = [list(available_conditions.keys())[i] for i in used_conditions]\n",
    "\n",
    "data_conditioned = data[applied_conditions] if used_conditions else data\n",
    "labels_conditioned = labels_x[applied_conditions] if used_conditions else labels_x\n",
    "print(\"Applied conditions:\",applied_conditions_names)\n",
    "print(\"Applied conditions length:\", len(data_conditioned) )\n",
    "\n",
    "removed_dict = {k:round(v,2) for (k,v) in zip(wave_i[~condition_skimmed], labels_x[~condition_skimmed])}\n",
    "removed_dict1 = {k:round(v,2) for (k,v) in zip(wave_i[~condition_i_slope], labels_x[~condition_i_slope])}\n",
    "removed_dict2 = {k:v for k,v in removed_dict.items() if v > 10}\n",
    "removed_dict3 = {k:round(v,2) for (k,v) in zip(wave_i[~condition_f40], labels_x[~condition_f40])}\n",
    "# if removed_dict2:\n",
    "#     data_df[[col for col in data_df.columns if int(col[col.find(\"[\")+1:col.find(\"]\")]) in removed_dict2.keys()][:100]].iplot(title=f\"Removed waveforms (xT <= {cutoff_xT})\")\n",
    "# elif removed_dict:\n",
    "#     data_df[[col for col in data_df.columns if int(col[col.find(\"[\")+1:col.find(\"]\")]) in removed_dict.keys()][:100]].iplot(title=f\"Removed waveforms (xT <= {cutoff_xT})\")\n",
    "data_df[[col for col in data_df.columns if int(col[col.find(\"[\")+1:col.find(\"]\")]) in removed_dict1.keys()][:100]].iplot(title=f\"Removed waveforms (i_10_slope > {cutoff_i_slope})\")\n",
    "data_df[[col for col in data_df.columns if int(col[col.find(\"[\")+1:col.find(\"]\")]) in removed_dict3.keys()][:100]].iplot(title=f\"Removed waveforms (Final 40 < 1.125)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3beaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"Sklearn NN, Na22 th.ALL Ch0\"\n",
    "# run_name = f\"NN{str(regr.hidden_layer_sizes).replace(',',' ')}_{(l_train.shape[0]+l_test.shape[0])/len(data_df.columns):.2%}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")[2:]}\"\n",
    "# print(run_name)\n",
    "\n",
    "def save_experiment(exp_name, run_name, data_df, data_name, train_set, test_set, train_labels_t, test_labels_t, applied_conditions_names, fitted_regressor, pca_components):\n",
    "    experiment = mlflow.set_experiment(exp_name)\n",
    "    regr = fitted_regressor\n",
    "\n",
    "    source_path = os.path.join(os.path.abspath(\"./data\"),data_name)\n",
    "    dataset_train: NumpyDataset = from_numpy(train_set, source=source_path, name=data_name+\" train\", targets=t_to_x(train_labels_t))\n",
    "    dataset_test: NumpyDataset = from_numpy(test_set, source=source_path, name=data_name+\" test\", targets=t_to_x(test_labels_t))\n",
    "\n",
    "    # Add histogram with predicted vs actual data\n",
    "    s_labels_t = pd.Series(np.append(train_labels_t,test_labels_t))\n",
    "    shift = -round(s_labels_t.mean())\n",
    "    s_labels_t.name = f\"Initial data: dT {'-' if shift < 0 else '+'} {abs(shift)} ns\"\n",
    "    pred_s_train = pd.Series(train_labels_t - x_to_t(regr.predict(train_set)),name=\"dT_act - dT_pred (train)\")\n",
    "    pred_s_test = pd.Series(test_labels_t - x_to_t(regr.predict(test_set)),name=\"dT_act - dT_pred (test)\")\n",
    "    fig, params = histogram(pd.concat([s_labels_t+shift,pred_s_train,pred_s_test], axis=1), [-30,30,0.25], True, title=\"Arrival Time Histogram\", xaxis_title=\"Time (ns)\", yaxis_title=\"Frequency\")\n",
    "    fwhm_train = 2*np.sqrt(2*np.log(2)) * params[pred_s_train.name][2]\n",
    "    fwhm_test = 2*np.sqrt(2*np.log(2)) * params[pred_s_test.name][2]\n",
    "    fig.show()\n",
    "    with mlflow.start_run(run_name=run_name) as mlflow_run:\n",
    "        mlflow.set_experiment_tag(\"BaseModel\",\"SKLearn Neural Network MLPRegressor\")\n",
    "        mlflow.log_params({\n",
    "            \"PCA components\": pca_components,\n",
    "            \"Hidden layers\": str(regr.hidden_layer_sizes).replace(\",\",\" \"), \n",
    "            \"Activation function\":regr.activation,\n",
    "            \"Solver\": regr.solver,\n",
    "            \"Alpha\": regr.alpha,\n",
    "            \"Max epochs\": regr.max_iter,\n",
    "            \"Waveforms used\": f'{train_labels_t.shape[0]+test_labels_t.shape[0]} / available {len(data_df.columns)}',\n",
    "            \"Data train - data test shapes\": f'{train_labels_t.shape} - {test_labels_t.shape}'.replace(\"(\",\"/\").replace(\")\",\"/\").replace(\",\",\" \"),\n",
    "            \"Applied conditions\": str(applied_conditions_names).replace(\",\",\"  \") or \"None\"\n",
    "        })\n",
    "        mlflow.log_figure(fig, \"PredictionHistogram.html\")\n",
    "        fig = go.Figure(go.Scatter(y=regr.loss_curve_,name=\"Loss Curve\"))\n",
    "        fig.add_trace(go.Scatter(x=[regr.loss_curve_.index(regr.best_loss_)],y=[regr.best_loss_],name=\"Loss minimum\"))\n",
    "        fig.update_layout(title='Loss curve plot',\n",
    "                        xaxis_title=\"Epoch\",\n",
    "                        yaxis_title=\"Loss\")\n",
    "        mlflow.log_figure(fig, \"LossCurve.html\")\n",
    "        mlflow.log_metrics({\n",
    "            \"FWHM Train\": fwhm_train,\n",
    "            \"FWHM Test\": fwhm_test,\n",
    "            \"Loss final\": regr.loss_,\n",
    "            \"Loss min.\": regr.best_loss_, #f\"{regr.best_loss_:.2f} - {str(regr.loss_curve_.index(regr.best_loss_))}\"\n",
    "            \"Loss min. epoch\": regr.loss_curve_.index(regr.best_loss_),\n",
    "            \"Validation score R2\": regr.score(test_set, t_to_x(test_labels_t)),\n",
    "            \"Iterations/epochs\": regr.n_iter_,\n",
    "            \"t\": regr.t_,\n",
    "            \"Train mean\":pred_s_train.mean(),\n",
    "            \"Train RMS\": pred_s_train.std(),\n",
    "            \"Test mean\": pred_s_test.mean(),\n",
    "            \"Test RMS\": pred_s_test.std()\n",
    "        })\n",
    "        mlflow.log_input(dataset_train, context=\"training\")\n",
    "        mlflow.log_input(dataset_test, context=\"testing\")\n",
    "        # mlflow.log_artifact(\"model.png\",\"model_plot\")\n",
    "        mlflow.sklearn.log_model(regr, \"MLPRegressorModel\")\n",
    "        # mlflow.keras.log_model(regr, \"MLPRegressorModel\")\n",
    "        print(\"MLFlow run ID:\", mlflow_run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7a175-0e50-4400-8149-7648a79cdcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterate_through = [list(zip(data_dict.keys(), data_dict.values()))[3]] # data_dict.items()\n",
    "\n",
    "for data_name, used_data in data_dict.items():\n",
    "    single_layers = [[3], [4], [8], [12], [16], [32], [64]] # 7 * 20s = ~2 mins (All 7 of them take 14 mins)\n",
    "    dual_layers = [[3,3], [4,4], [8,8], [4,8], [8,4], [4,16], [8,16], [16,4], [16,8], [8,32], [32,8], [16,16], [16,16], [16,64], [64,16]] # 15 * 2 mins = ~30 mins\n",
    "    triple_layers = []\n",
    "    for hidden_layers in [[16]]:\n",
    "        print(\"Running experiment:\",data_name,hidden_layers,\"...\")\n",
    "        data_df = get_waveforms(0, len(used_data), source_data=used_data, get_indices_map=False)#[10:]\n",
    "        # display(data_df.head())\n",
    "        # data_df_last_10 = data_df[-10:].diff().mean().values #_fit_final_slope(data_df.values, data_df.columns).flatten()\n",
    "        # print(len(data_df_last_10),\"long array\",data_df_last_10)#,\"\\n\", _fit_final_slope(data_df.values, data_df.columns).flatten())\n",
    "        data = data_df.values.transpose()\n",
    "        labels_t = np.array([float([s[s.find(\"]\")+1:s.find(\"dT\")] for s in [col.replace(\" \",\"\")]][0]) for col in data_df.columns])\n",
    "        labels_x = t_to_x(labels_t)\n",
    "        wave_i = np.array([int(col[col.find(\"[\")+1:col.find(\"]\")]) for col in data_df.columns])\n",
    "\n",
    "        n_components = 64\n",
    "        model = PCA(n_components)\n",
    "        data_trans = model.fit_transform(data)\n",
    "        pca_var_ratio = model.explained_variance_ratio_\n",
    "\n",
    "        # past_cutoff_series = df_results[\"x_cutoff\"] - df_results[\"xT\"]\n",
    "        # par_fit_var_series = df_results[\"x0_cutoff_var\"]\n",
    "        # total_var = df_results[\"var\"]\n",
    "        # exp_99_series = df_results[\"exp_x_99%\"]\n",
    "        # b_series = df_results[\"b\"]\n",
    "        # c_series = df_results[\"c\"]\n",
    "        # colors = {\"before<br>cutoff\":past_cutoff_series.values}|{s.name:s.values for s in [par_fit_var_series,total_var,exp_99_series,b_series,c_series]}|{\"slope<br>last 10\":data_df_last_10}\n",
    "        # past_cutoff = 3\n",
    "\n",
    "        # conditions_sk2 = (labels_x > cutoff_xT) & (past_cutoff_series > past_cutoff)\n",
    "\n",
    "        d_train, d_test, l_train, l_test, l_train_t, l_test_t, wi_train, wi_test = train_test_split_cond(data_trans, labels_x, labels_t, wave_i, test_size=0.5, \n",
    "                                                                                                        random_state=42)\n",
    "\n",
    "        # d_train2, d_test2, l_train2, l_test2, l_train_t2, l_test_t2, wi_train2, wi_test2 = train_test_split_cond(data_trans, labels_x, labels_t, wave_i, test_size=0.5, \n",
    "        #                                                                                                           random_state=42, conditions=conditions_sk2)\n",
    "        print(\"train, test shapes:\",l_train.shape, l_test.shape, \" -> total data set of\", l_train.shape[0]+l_test.shape[0], \"out of possible\",len(data_df.columns))\n",
    "        plot_transform(data_trans, n_components, 1)\n",
    "\n",
    "        regr = MLPRegressor(hidden_layer_sizes = hidden_layers,\n",
    "                            activation = \"relu\",\n",
    "                            solver = \"adam\",\n",
    "                            alpha = 1e-4,\n",
    "                            max_iter = 10000)\n",
    "\n",
    "\n",
    "        # regr2 = MLPRegressor(hidden_layer_sizes = [16],\n",
    "        #                     activation = \"relu\",\n",
    "        #                     solver = \"adam\",\n",
    "        #                     alpha = 1e-4,\n",
    "        #                     max_iter = 1000)\n",
    "\n",
    "        # training model\n",
    "        regr.fit(d_train, l_train)\n",
    "\n",
    "        # training model\n",
    "        # regr2.fit(d_train2, l_train2)\n",
    "        s_labels_t = pd.Series(np.append(l_train_t,l_test_t))\n",
    "        shift = -round(s_labels_t.mean())\n",
    "        s_labels_t.name = f\"Initial data: dT {'-' if shift < 0 else '+'} {abs(shift)} ns\"\n",
    "\n",
    "        pred_s_train = pd.Series(l_train_t - x_to_t(regr.predict(d_train)),name=\"dT_act - dT_pred (train)\")\n",
    "        pred_s_test = pd.Series(l_test_t - x_to_t(regr.predict(d_test)),name=\"dT_act - dT_pred (test)\")\n",
    "\n",
    "        fig, params = histogram(pd.concat([s_labels_t+shift,pred_s_train,pred_s_test], axis=1), [-30,30,0.25], True, title=\"Arrival Time Histogram\", xaxis_title=\"Time (ns)\", yaxis_title=\"Prevalence\")\n",
    "        fwhm_train = 2*np.sqrt(2*np.log(2)) * params[pred_s_train.name][2]\n",
    "        fwhm_test = 2*np.sqrt(2*np.log(2)) * params[pred_s_test.name][2]\n",
    "        fig.show()\n",
    "\n",
    "        name_signature = data_name[-15:][data_name[-16:].find(\"-\"):-4]\n",
    "        experiment_name = f\"Sklearn NN, Na22 {'th.'+name_signature[-2:] if 'ecf' in name_signature else name_signature} Ch0\"\n",
    "        run_name = f\"NN{str(regr.hidden_layer_sizes).replace(',',' ')}_{(l_train.shape[0]+l_test.shape[0])/len(data_df.columns):.2%}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")[2:]}\"\n",
    "\n",
    "        print(\"Saving experiment:\",experiment_name,\", with run:\",run_name)\n",
    "        save_experiment(exp_name=experiment_name,\n",
    "                        run_name=run_name,\n",
    "                        data_df=data_df,\n",
    "                        data_name=data_name,\n",
    "                        train_set=d_train,\n",
    "                        test_set=d_test,\n",
    "                        train_labels_t=l_train_t,\n",
    "                        test_labels_t=l_test_t,\n",
    "                        applied_conditions_names=applied_conditions_names,\n",
    "                        fitted_regressor=regr,\n",
    "                        pca_components=n_components\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcb7715-ccf5-4b92-8609-312b411137d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graphs = False\n",
    "\n",
    "# past_cutoff_results = df_results[df_results[\"x_cutoff\"] < df_results[\"xT\"]]\n",
    "# display(df_results[df_results[\"xT\"] > cutoff_xT])\n",
    "# display(past_cutoff_series_skimmed) # Positive is good, negative is bad\n",
    "print(f\"20 t is {t_to_x(20)} x, 60 t is {t_to_x(60)} x, -20 t is {t_to_x(-20)} x, 160 t is {t_to_x(160)} x, 186 t is {t_to_x(186)} x\")\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"1: {len(data_trans[condition_skimmed])}/{len(data0)} ({len(data_trans[condition_skimmed])/len(data0):.1%})\")\n",
    "# print(f\"2: {len(data_trans[conditions_sk2])}/{len(data0)} ({len(data_trans[conditions_sk2])/len(data0):.1%})\")\n",
    "print(\"color order:\", {i:k for i,k in enumerate(colors.keys())})\n",
    "color_array, color_name = [(a,n) for n,a in colors.items()][4] #par_fit_var_series_skimmed.values #last_10_skimmed\n",
    "color_test, color_train = train_test_split_cond(color_array, test_size=0.5, random_state=42, conditions=condition_skimmed) \n",
    "# color_test2, color_train2 = train_test_split_cond(color_array, test_size=0.5, random_state=42, conditions=conditions_sk2)\n",
    "c_min_max_var = (0.000002, 0.000015)\n",
    "add_kwargs = {\"colorbar_name\":color_name, \"xaxis_title\":\"Predicted dT\",\"yaxis_title\":\"Measured dT\", \"c_min_max\": (0.0001, 0.015)}\n",
    "add_kwargs.pop(\"c_min_max\")\n",
    "\n",
    "fig_train = mlp_reg_fig(d_train, l_train_t, regr, wi_train, regr_wrapper=x_to_t, color=color_test, **add_kwargs)\n",
    "# print(labels_test,labels_removed)\n",
    "# new_data_test = np.append(data_test,data_removed,axis=0)\n",
    "# new_labels_test = np.append(labels_test,labels_removed)\n",
    "fig_test = mlp_reg_fig(d_test, l_test_t, regr, wi_test, regr_wrapper=x_to_t, color=color_train, **add_kwargs)\n",
    "# fig_train2 = mlp_reg_fig(d_train2, l_train_t2, regr2, wi_train2, regr_wrapper=x_to_t, color=color_test2, **add_kwargs)\n",
    "# fig_test2 = mlp_reg_fig(d_test2, l_test_t2, regr2, wi_test2, regr_wrapper=x_to_t, color=color_train2, **add_kwargs)\n",
    "# [fig.update_layout(title=\"Predicted and actual dT\") for fig in [fig_train, fi\n",
    "# g_test, fig_train2, fig_test2]]\n",
    "# [fig.update_layout(title=f\"Predicted and actual dT (removed past cutoff >{past_cutoff})\") for fig in [fig_train2, fig_test2]]\n",
    "\n",
    "# display(df_results.loc[[f\"[{i}]\" for i in wi_train[np.abs(l_train - regr.predict(d_train)) < 4]]].mean())\n",
    "# df_results.loc[[f\"[{i}]\" for i in wi_train[np.abs(l_train - regr.predict(d_train)) < 4]]].drop(columns=[\"xT\",\"yT_fit\"]).hist()\n",
    "# display(df_results.loc[[f\"[{i}]\" for i in wi_train[np.abs(l_train - regr.predict(d_train)) > 4]]].mean())\n",
    "# df_results.loc[[f\"[{i}]\" for i in wi_train[np.abs(l_train - regr.predict(d_train)) > 4]]].drop(columns=[\"xT\",\"yT_fit\"]).hist()\n",
    "\n",
    "fig_train.show() if show_graphs else None\n",
    "fig_test.show() if show_graphs else None\n",
    "# fig_train2.show() if show_graphs else None\n",
    "# fig_test2.show() if show_graphs else print(\"(No graphs shown)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb4d2c-3fa3-42f4-8ceb-85840e1058df",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
