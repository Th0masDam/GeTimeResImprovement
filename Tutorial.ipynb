{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf4027f7-e8c6-4047-aacf-b7d0b1d746d1",
   "metadata": {},
   "source": [
    "# Tutorial notebook\n",
    "\n",
    "This notebook describes how to train Neural Network Regressors and Extreme Gradient Boosted Regression Trees using GeWaPro in the following paragraphs:\n",
    "1. Loading and visualizing the (training) data\n",
    "2. Setting up MLFlow experiments\n",
    "3. Retrieving & interpreting test results\n",
    "4. Using saved models to do predictions\n",
    "5. Exporting MLFlow models for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d8d53b-fcc5-4221-9e41-916e6f465935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly import graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "from IPython.display import HTML\n",
    "import cufflinks\n",
    "import numba as nb\n",
    "from scipy.optimize import least_squares, curve_fit\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import mlflow\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import mlflow.keras\n",
    "import mlflow.sklearn\n",
    "from gewapro.cache import cache\n",
    "from gewapro.preprocessing import get_waveforms, train_test_split_cond, smoothen_waveforms, get_and_smoothen_waveforms, select_from_source\n",
    "from gewapro.functions import (quadratic_arr,\n",
    "                               fit_parabolas,\n",
    "                               df_with_fits,\n",
    "                               _fit_final_slope,\n",
    "                               combine_and, combine_or,\n",
    "                               calc_ab)\n",
    "from gewapro.plotting.base import _fwhm_energy_df\n",
    "from gewapro.util import name_to_vals, pandas_string_rep, add_notes, combine_cols_with_errors\n",
    "from gewapro.plotting import (histogram,\n",
    "                              corr_fig,\n",
    "                              mlp_reg_fig,\n",
    "                              plot_transform,\n",
    "                              energy_histogram,\n",
    "                              box_plot,\n",
    "                              plot_predictions,\n",
    "                              energy_line_plot,\n",
    "                              add_energy_histogram,\n",
    "                              combine_line_plots,\n",
    "                              combined_channel_line_plot,\n",
    "                              change_combined_line_fig)\n",
    "from gewapro import plotting\n",
    "from gewapro.models import regressor_model, train_model, get_model_version_map, ModelInfo, fitted_PCA\n",
    "import gewapro.models\n",
    "from gewapro.experiment_flow import run_experiment\n",
    "import mlflow.pyfunc\n",
    "import xgboost as xgb\n",
    "import itertools\n",
    "\n",
    "cufflinks.go_offline()\n",
    "\n",
    "# If a new model is added to the saved models, rerun this function below:\n",
    "gewapro.models.update_validity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec2811",
   "metadata": {},
   "source": [
    "## 1. Loading and visualizing the (training) data\n",
    "Data for training and testing can be visualized below. See code comments for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abde9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "data_g1274_name = \"20231110-Na22-d0-12-tz6-ML200-g1274.dat\"  # Gate 1274, detector 0 data\n",
    "data_g1274 = pd.read_csv(\"data/\"+data_g1274_name)\n",
    "data_g511_name = \"20231110-Na22-d0-12-tz6-ML200-g511.dat\"  # Gate 511, detector 0 data\n",
    "data_g511 = pd.read_csv(\"data/\"+data_g511_name)\n",
    "\n",
    "# Rename Tref to Tfit in columns, not needed if Tfit is already present in columns\n",
    "data_g511.columns = [(\"Tfit\" if col == \"Tref\" else col) for col in data_g511.columns]\n",
    "data_g1274.columns = [(\"Tfit\" if col == \"Tref\" else col) for col in data_g1274.columns]\n",
    "\n",
    "# Define a dictionary that holds al data, useful later\n",
    "data_dict = {\n",
    "    data_g1274_name : data_g1274,\n",
    "    data_g511_name  : data_g511,\n",
    "  # your data name  : your custom data set,\n",
    "\n",
    "  # For example, a set with start value between (-0.15,0.15) & final value between (0.85,1.15):\n",
    "    \"20231110-Na22-d0-tz6-ML200-g511-tol0.15\" : data_g511[(data_g511[\"s0\"]  > -0.15) & \\\n",
    "                                                          (data_g511[\"s0\"]  <  0.15) & \\\n",
    "                                                          (data_g511[\"s199\"] > 0.85) & \\\n",
    "                                                          (data_g511[\"s199\"] < 1.15) ],\n",
    "}\n",
    "\n",
    "# Two simple functions that convert the waveform index (x) to time units in ns (t) or back\n",
    "x_to_t = lambda x: 160-(x*4)\n",
    "t_to_x = lambda t: (160-t)/4\n",
    "\n",
    "# Show original data in table form\n",
    "print(f\"Original data ({len(data_g511)} waveforms):\")\n",
    "display(data_g511)\n",
    "print(f\"Dataset length of tolerance .15 data:\", len(data_dict[\"20231110-Na22-d0-tz6-ML200-g511-tol0.15\"]), \"waveforms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is transposed so it can be plotted & used as training data with the get_waveforms(...) function:\n",
    "df_waveforms = get_waveforms(select_channels=0, source_data=data_g511)\n",
    "print(\"Data after waveform transformation (each column is a single waveform for training):\")\n",
    "display(df_waveforms)\n",
    "print(\"total waveform count:\", len(df_waveforms.columns))\n",
    "\n",
    "# PLot the data, first 100 waveforms between .85 & .95, and below .85 final data point\n",
    "display(HTML(\"<h2>1.1 Displaying the first 100 waveforms (with various imposed conditions):</h2>\"))\n",
    "df_waveforms.iloc[:,-100:].set_index(df_waveforms.index*4).iplot(title=\"Last 100 waveforms (no conditions)\",theme=\"white\",xaxis_title=\"time [ns]\")\n",
    "dfplot85_95: pd.DataFrame = df_waveforms.loc[:, (df_waveforms.loc[199] > 0.85) & (df_waveforms.loc[199] < 0.95)]\n",
    "dfplot85_95.iloc[:,:100].set_index(dfplot85_95.index*4).iplot(title=\"First 100 waveforms with last value between 0.85 and 0.95\",theme=\"white\",xaxis_title=\"time [ns]\")\n",
    "dfplot85: pd.DataFrame = df_waveforms.loc[:,df_waveforms.loc[199] < 0.85]\n",
    "dfplot85.iloc[:,:100].set_index(dfplot85.index*4).iplot(title=\"First 100 waveforms with last value below 0.85\",theme=\"white\",xaxis_title=\"time [ns]\")\n",
    "\n",
    "# Plots can be saved to pdf, png or other with the Plotly Figure.write_image(...) method:\n",
    "fig: go.Figure = dfplot85_95.iloc[:,:100].set_index(dfplot85.index*4).iplot(asFigure=True,theme=\"white\").update_layout(height=400,width=700,margin=dict(l=20, r=20, t=20, b=20),showlegend=False,xaxis_title=\"time [ns]\",yaxis_title=\"Normalised signal\")\n",
    "fig.write_image(\"Example_waveforms_plot_85-95perc-finalpoint.pdf\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10dffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of all Tfit timings can be plotted via the histogram(...) function\n",
    "# By default, a Gaussian fit is included, but it can also be excluded by changing the parameter add_fits from ['Gaussian'] to [].\n",
    "display(HTML(\"<h2>1.2 The distribution of all Tfit (initial data) timings:</h2>\"))\n",
    "plot_title = f\"Histogram of Tfit timings for dataset '{data_g511_name}'\"\n",
    "histogram(data=data_g511[\"Tfit\"], bins=[-60,30,0.5], mode=\"Bar\", title=plot_title+\" (binwidth 0.5, mode 'Bar')\", xaxis_title=\"time [ns]\").show()\n",
    "histogram(data=data_g511[\"Tfit\"], bins=[-60,30,2], mode=\"Line\", title=plot_title+\" (binwidth 2.0, mode 'Line')\", xaxis_title=\"time [ns]\").show()\n",
    "hist = histogram(data=data_g511[\"Tfit\"], bins=[-60,30,1], add_fits=[\"Inv. Quadratic\"], title=plot_title+\" (binwidth 1.0, mode 'DEFAULT')\", xaxis_title=\"time [ns]\")\n",
    "hist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the default plotting settings can be done using the gewapro.plotting.settings(...) function\n",
    "print(\"Unchanged default settings:\", plotting.settings.reset())\n",
    "# From here we set the default_plot_mode for histograms to 'Line':\n",
    "plotting.settings(default_plot_mode='Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfe491",
   "metadata": {},
   "source": [
    "## 2. Setting up MLFlow experiments\n",
    "\n",
    "The ``run_experiment(...)`` function can be used to train a model and run & log it inside an MLFlow experiment.\n",
    "\n",
    "**!!! First, run ``mlflow ui`` in a terminal in this folder, so that the experiment runs with trained models can be saved !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d597ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this, a tracking URI can be set\n",
    "tracking_uri = \"http://127.0.0.1:5000\"  # It is known that on Windows the default /localhost/ URI is slow, so use 127.0.0.1 instead\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "print(\"[INFO] Set MLFlow tracking URI to:\",tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8ebd2",
   "metadata": {},
   "source": [
    "### 2.1 Single experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a45cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run single experiment\n",
    "model_type = \"SKLearn\"  \n",
    "uniform_test_set = [5000,6000,7000,8000,9000,10000,11000,12000,13000,15000]\n",
    "data_g511_tol015 = \"20231110-Na22-d0-tz6-ML200-g511-tol0.15\"\n",
    "\n",
    "result_single_exp = run_experiment(\n",
    "    experiment_name = \"Test Experiment\",\n",
    "  # run_name = custom run name,           # The run name defaults to a combination of training data, date & time\n",
    "    data = data_dict[data_g511_tol015],   # Using the data dict here to get the DataFrame \"data_g511\", not yet turned into waveforms\n",
    "    data_name = data_g511_tol015,         # Name of the data, used for tracking/logging\n",
    "    select_channels = [] ,                # All channels available, so only channel 0 for this data set\n",
    "    select_energies = (5000,15000),       # From arbitrary units 5000 - 15000\n",
    "    include_energy = False,               # Whether to include final amplitude in training, default: False\n",
    "    pca_components = None,                # No PCA components, this means no PCA is used\n",
    "    pca_method = TruncatedSVD,            # Which PCA method to use (if components is set), by default sklearn.decomposition.PCA\n",
    "    model_type = model_type,              # \"SKLearn\" or \"TensorFlow\" or \"XGBoost\"\n",
    "    test_size = 0.3,                      # Testing set size compared to total data set\n",
    "    uniform_test_set = uniform_test_set,  # Uniform extra test sets of energies 5000-6000, 6000-7000, etc.\n",
    "    registered_model_name = \"TestModel\",  # Name of the model in the MLFlow registry, set this to \"auto\" to not worry about it\n",
    "  # Only valid for Neural Networks:\n",
    "    hidden_layers = [23],                 # Hidden layers and amount of neurons, e.g. [16,34] is a two-layer model with 16 and 34 neurons each\n",
    "    alpha = 1e-4,                         # Alpha regularisation parameter or 'training strictness': the lower, the stricter\n",
    "    max_iterations = 1000,                # Max number of training iterations, by default: 2000\n",
    "    activation = \"relu\",                  # Neuron activation function: \"relu\" is the rectified linear function: 0 for negative x, x for positive x\n",
    ")\n",
    "\n",
    "# The run_experiment function outputs a prediction histogram by default, which can be shown:\n",
    "result_single_exp.show()\n",
    "print(\"Experiment result parameters:\", result_single_exp._params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560d9ff",
   "metadata": {},
   "source": [
    "### 2.2 Loops of experiments\n",
    "A bunch of experiment repetitions can also be done with for-loops. For example, in the case of XGBoosted Regression Trees, testing the amount of regressors, regularisation, max tree depth, branching factor etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e9a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run group of experiments\n",
    "model_type = \"XGBoost\"\n",
    "uniform_test_set = [5000,6000,7000,8000,9000,10000] # Test sets with energy ranges 5000-6000, 6000-7000, 7000-8000, 8000-9000 & 9000-10000, of equal sample size\n",
    "data_g511_tol015 = \"20231110-Na22-d0-tz6-ML200-g511-tol0.15\"\n",
    "\n",
    "# The lists\n",
    "iterations = 2     # 2 iterations per model configuration\n",
    "pca_comps_list = [16, 32, None]\n",
    "max_tree_depth_list = [20, 30]\n",
    "number_of_regressors_list = [3, 5]\n",
    "total_iterations = iterations*len(pca_comps_list)*len(max_tree_depth_list)*len(number_of_regressors_list)\n",
    "print(f\"Received {total_iterations} (= {iterations}*{len(pca_comps_list)}*{len(max_tree_depth_list)}*{len(number_of_regressors_list)}) experiments to run.\")\n",
    "\n",
    "# The group of experiment runs, takes about 1 min per iteration -> 24 iterations ~ 24 minutes\n",
    "i = 0\n",
    "for pca_comps in pca_comps_list:\n",
    "    for tree_depth in max_tree_depth_list:\n",
    "        for regressor_count in number_of_regressors_list:\n",
    "            for iteration in range(iterations):\n",
    "                i += 1\n",
    "                print(f\"Starting experiment {i}/{total_iterations} (iteration {iteration+1}/{iterations})...\")\n",
    "                result_single_exp = run_experiment(\n",
    "                    experiment_name = \"Looped Test Experiment\",\n",
    "                  # run_name = custom run name,             # The run name defaults to a combination of training data, date & time\n",
    "                    data = data_dict[data_g511_tol015],     # Using the data dict here to get the DataFrame \"data_g511\", not yet turned into waveforms\n",
    "                    data_name = data_g511_tol015,           # Name of the data, used for tracking/logging\n",
    "                    select_channels = [] ,                  # All channels available, so only channel 0 for this data set\n",
    "                    select_energies = (5000,15000),         # From arbitrary units 5000 - 15000\n",
    "                    include_energy = False,                 # Whether to include final amplitude in training, default: False\n",
    "                    pca_components = pca_comps,             # No PCA components, this means no PCA is used\n",
    "                    pca_method = PCA,                       # Which PCA method to use (if components is set), by default sklearn.decomposition.PCA\n",
    "                    model_type = model_type,                # \"SKLearn\" or \"TensorFlow\" or \"XGBoost\"\n",
    "                    test_size = 0.3,                        # Testing set size compared to total data set\n",
    "                    uniform_test_set = uniform_test_set,    # Uniform extra test sets of energies 5000-6000, 6000-7000, etc.\n",
    "                    registered_model_name = \"XGBTestModel\", # Name of the model in the MLFlow registry, set this to \"auto\" to not worry about it\n",
    "                  # Only valid for Regression Trees:\n",
    "                    max_depth = tree_depth,\n",
    "                    n_estimators = regressor_count,\n",
    "                    max_leaves = 0,\n",
    "                )\n",
    "print(\"Final experiment distribution graph:\")\n",
    "result_single_exp.show()\n",
    "\n",
    "# New models were added to the saved models, so we rerun the update_validity function\n",
    "gewapro.models.update_validity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0efb911",
   "metadata": {},
   "source": [
    "## 3. Retrieving & interpreting test results\n",
    "Now, after these first 2 experiments, going to [127.0.0.1:5000](http://127.0.0.1:5000) in browser, there should be two experiments shown in the left bar: *Test Experiment* & *Looped Test Experiment*. Clicking on these, all individual runs can be seen, with the models saved there also available. The *Test Experiment* should have only one trained *TestModel* (or as many times as you ran the single experiment), while the *Looped Test Experiment* should have 24 models available, called *XGBTestModel*.\n",
    "\n",
    "The performance of these models can be compared in a graph. All one needs is the experiment number, which is in the URL after ``experiments/``, such as in the URL\n",
    "``http://127.0.0.1:5000/#/experiments/595346769839476301?searchFilter=&orderByKey=...``. In this case, the experiment number is ``595346769839476301``.\n",
    "\n",
    "Graphs that are visually intuitive to use here are box plots, or distribution plots. The ``box_plot(...)`` function can retrieve the experiment results and plot the performance of all models in a single plot. The performance metric can be provided yourself via the ``y`` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdfef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put here the experiment ids from the URL:\n",
    "single_exp_id = 595346769839476301 #...\n",
    "looped_exp_id = 755987746823550296 #...\n",
    "\n",
    "# Show performance box plot of the two experiments\n",
    "box_plot(exp_id=[single_exp_id, looped_exp_id], x=\"PCA components\", y=\"FWHM Test\", color=\"Model name\", hover_name=\"model_version\").show()\n",
    "\n",
    "# Show performance box plot of the XGBTestModel looped experiment\n",
    "box_plot(exp_id=[looped_exp_id],\n",
    "         x=\"PCA components\",\n",
    "         y=\"FWHM Test\",\n",
    "         color=\"Max tree depth\",\n",
    "         facet_row=\"Number of estimators\",\n",
    "         height=700,\n",
    "         ignore_vals={\"Number of estimators\":None},\n",
    "         hover_name=\"model_version\").show()\n",
    "\n",
    "box_plot(exp_id=[looped_exp_id],\n",
    "         x=\"PCA components\",\n",
    "         y=\"Overtraining factor\",\n",
    "         color=\"Max tree depth\",\n",
    "         facet_row=\"Number of estimators\",\n",
    "         height=700,\n",
    "         ignore_vals={\"Number of estimators\":None},\n",
    "         hover_name=\"model_version\",\n",
    "         load_cols=\"FWHM Test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced metrics can also be made and used in the box plots\n",
    "def square_FWHM_metric(df: pd.DataFrame) -> pd.Series:\n",
    "    ranges = {\"5000-6000\":13.3579,\"6000-7000\":11.841,\"7000-8000\":10.3735,\"8000-9000\":10.2916,\"9000-10000\":9.4387,\"10000-11000\":11.2823,\"11000-12000\":7.0638}\n",
    "    return sum([(df[f\"metrics.Uniform test FWHM E{e_range}\"]/e_val)**2 for e_range,e_val in ranges.items()])\n",
    "\n",
    "def linear_FWHM_metric(df: pd.DataFrame) -> pd.Series:\n",
    "    ranges = {\"5000-6000\":13.3579,\"6000-7000\":11.841,\"7000-8000\":10.3735,\"8000-9000\":10.2916,\"9000-10000\":9.4387,\"10000-11000\":11.2823,\"11000-12000\":7.0638}\n",
    "    return sum([df[f\"metrics.Uniform test FWHM E{e_range}\"]/e_val for e_range,e_val in ranges.items()])\n",
    "linear_FWHM_metric.FWHM = square_FWHM_metric.FWHM = 8\n",
    "\n",
    "box_plot([single_exp_id],\n",
    "         x=\"PCA components\",\n",
    "         y=square_FWHM_metric,\n",
    "         color=\"Hidden layers\").show()\n",
    "\n",
    "box_plot([single_exp_id],\n",
    "         x=\"PCA components\",\n",
    "         y=linear_FWHM_metric,\n",
    "         color=\"Hidden layers\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb15040",
   "metadata": {},
   "source": [
    "## 4. Using saved models to do predictions\n",
    "Saved models from experiments can be retrieved with the ``ModelInfo`` class, which can be instantiated with the ``from_database`` method.\n",
    "\n",
    "### 4.1 Retrieving saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4576cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run 'mlflow ui' in a terminal, otherwise this will not work!!!\n",
    "test_model_1_info = ModelInfo.from_database(model_name=\"TestModel\",model_version=1)\n",
    "test_model_12_info = ModelInfo.from_database(model_name=\"XGBTestModel\",model_version=12)\n",
    "print(test_model_1_info)\n",
    "print(test_model_12_info)\n",
    "\n",
    "# The models themselves and their PCA classes can be extracted\n",
    "model_12 = test_model_12_info.model\n",
    "pca_model_12 = test_model_12_info.get_transformer()\n",
    "print(model_12)\n",
    "pca_model_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions can be done with plot_predictions(...), note that the PCA parameters must be exactly the same as before, so fitted on the same training data\n",
    "model_version = 22  # Can be varied to plot for different model versions\n",
    "pca_fit_model = fitted_PCA(model_name = \"XGBTestModel\",\n",
    "                           model_version = model_version,\n",
    "                           waveforms = get_waveforms(source_data=data_dict[\"20231110-Na22-d0-tz6-ML200-g511-tol0.15\"]))\n",
    "\n",
    "# Test predictions on other data set (1274)\n",
    "plot_predictions(on_data = data_g1274_name,\n",
    "                 energy_range = (),\n",
    "                 model_version = model_version,\n",
    "                 data_dict = data_dict,\n",
    "                 model_name = \"XGBTestModel\",\n",
    "                 PCA_fit = pca_fit_model,\n",
    "                 plot = \"Histogram\")\n",
    "\n",
    "# Same model but on original data set (511), should be better than new data set\n",
    "plot_predictions(on_data = data_g511_name,\n",
    "                 energy_range = (),\n",
    "                 model_version = model_version,\n",
    "                 data_dict = data_dict,\n",
    "                 model_name = \"XGBTestModel\",\n",
    "                 PCA_fit = pca_fit_model,\n",
    "                 plot = \"Histogram\")\n",
    "\n",
    "# SKLearn NN model on other data set (1274)\n",
    "plot_predictions(on_data = data_g1274_name,\n",
    "                 energy_range = (),\n",
    "                 model_version = 1,\n",
    "                 data_dict = data_dict,\n",
    "                 model_name = \"TestModel\",\n",
    "                 plot = \"Histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b1532b",
   "metadata": {},
   "source": [
    "### 4.2 Energy line plots & energy histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c345d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy corrections and histogram\n",
    "e_corrections = {0: calc_ab(4477,11197),\n",
    "                 1: calc_ab(4623,11538),\n",
    "                 2: calc_ab(4212,10512),\n",
    "                 3: calc_ab(4672,11662),\n",
    "                 4: (1,0),  # LaBr channel\n",
    "                 5: calc_ab(1582,3948),\n",
    "                 6: (1,0),  # LaBr channel\n",
    "                 7: calc_ab(4747,11866),\n",
    "                 8: calc_ab(4303,10727),\n",
    "                 9: calc_ab(4750,11861),\n",
    "                 10:calc_ab(4113,10268),\n",
    "                 11: calc_ab(4474,11157)}\n",
    "\n",
    "fig_ehist = energy_histogram(data_g511_name, data_dict, select_energies=(0,1300), bins=[0,1400,2], correct_energy=e_corrections[0], xaxis_title=\"Energy [keV]\",\n",
    "                             title=f\"Predictions of XGBTestModel on channel 0 (trained on channel 0)\", colors=[\"rgba(0,0,0,0.5)\"])\n",
    "\n",
    "# Performance of a model on the energy range of the data can be visualized with energy_line_plot\n",
    "fig_eline = energy_line_plot(on_data = data_g1274_name,\n",
    "                             start = 50,\n",
    "                             end = 1250,\n",
    "                             step = 100,\n",
    "                             model_version = model_version,\n",
    "                             data_dict = data_dict,\n",
    "                             model_name = \"XGBTestModel\",\n",
    "                             PCA_fit = pca_fit_model,\n",
    "                             hist_limit = 34,\n",
    "                             correct_energy = e_corrections[0])\n",
    "\n",
    "add_energy_histogram(fig_eline=fig_eline, fig_ehist=fig_ehist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32057af",
   "metadata": {},
   "source": [
    "### 4.3 More plotting tools\n",
    "Combined channel line plots can combine predictions of different channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367386b9",
   "metadata": {},
   "source": [
    "## 5. Exporting MLFlow models for use\n",
    "Soon to be added..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f554a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f04fe",
   "metadata": {},
   "source": [
    "## A. Appendix: Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d40af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching is used a lot in the functions under the hood\n",
    "# Info on cache can be gotten with the .cache_info() method for all functions that have a @cache decorator\n",
    "print(energy_line_plot.cache_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb4d2c-3fa3-42f4-8ceb-85840e1058df",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
